\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{NormalizingFlowsFinal}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Enhancing Variational Autoencoder with Normalizing
Flows:}\label{enhancing-variational-autoencoder-with-normalizing-flows}

\section{A Multivariate g-k Distribution and Levenberg-Marquardt
Optimization Approach for MNIST Data Generation and
Reconstruction}\label{a-multivariate-g-k-distribution-and-levenberg-marquardt-optimization-approach-for-mnist-data-generation-and-reconstruction}

    \subsubsection{Overview}\label{overview}

This project enhances a Variational Autoencoder (VAE) with Normalizing
Flows using a Multivariate g-k Distribution and Levenberg-Marquardt (LM)
optimization, applied to the MNIST dataset.

\paragraph{Key Innovations:}\label{key-innovations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Multivariate g-k Distribution}:

  \begin{itemize}
  \tightlist
  \item
    Integrated as the base distribution in Normalizing Flows, the g-k
    distribution allows for flexible modeling of the latent space,
    capturing complex data structures more effectively than a standard
    normal distribution.
  \end{itemize}
\item
  \textbf{Levenberg-Marquardt Optimization}:

  \begin{itemize}
  \tightlist
  \item
    The g-k distribution parameters are optimized using LM, a precise
    method for non-linear parameter estimation. This optimization
    improves the flow's ability to model intricate patterns in the data,
    leading to better generation and reconstruction performance.
  \end{itemize}
\end{enumerate}

\paragraph{Implementation Highlights:}\label{implementation-highlights}

\begin{itemize}
\tightlist
\item
  \textbf{VAE Structure}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Encoder}: Compresses input images to a latent space.
  \item
    \textbf{Normalizing Flows}: Transforms the latent space using
    invertible flows, with the g-k distribution as the base.
  \item
    \textbf{Decoder}: Reconstructs images from the transformed latent
    space.
  \end{itemize}
\item
  \textbf{Training}:

  \begin{itemize}
  \tightlist
  \item
    The VAE is trained on MNIST, with the enhanced latent space enabling
    superior image generation.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

This approach focuses on the advanced use of the Multivariate g-k
Distribution and LM optimization, making the VAE more expressive and
capable of generating realistic, complex data patterns.

    \subsubsection{Introduction to Normalizing
Flows}\label{introduction-to-normalizing-flows}

Normalizing Flows are techniques used to transform a simple probability
distribution into a more complex one through a series of invertible
transformations, enhancing the model's ability to capture intricate data
patterns.

\paragraph{Key Concepts:}\label{key-concepts}

\begin{itemize}
\tightlist
\item
  \textbf{Base Distribution}: Typically starts with a simple
  multivariate Gaussian distribution.
\item
  \textbf{Flow Layers}: A sequence of invertible transformations applied
  to the base distribution to create a more complex distribution.
\item
  \textbf{Invertibility and Jacobian}: Ensures transformations are
  reversible and computationally efficient.
\end{itemize}

\subsubsection{Enhanced VAEs with Multivariate g-k Distribution and LM
Optimization}\label{enhanced-vaes-with-multivariate-g-k-distribution-and-lm-optimization}

In this approach, the base distribution in Normalizing Flows is replaced
with a \textbf{Multivariate g-k Distribution}. The parameters of this
distribution are optimized using \textbf{Levenberg-Marquardt (LM)},
leading to a more expressive latent space and improved generative
performance in VAEs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Data Preparation}

\PY{c+c1}{\PYZsh{} We start by loading and preprocessing the MNIST dataset.}

\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{DataLoader}
\PY{k+kn}{from} \PY{n+nn}{torchvision} \PY{k+kn}{import} \PY{n}{datasets}\PY{p}{,} \PY{n}{transforms}


\PY{n}{transform} \PY{o}{=} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
    \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{transforms}\PY{o}{.}\PY{n}{Normalize}\PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{p}{)}\PY{p}{)} 
\PY{p}{]}\PY{p}{)}


\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{test\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}


\PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{test\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Implementing the basic VAE model}
\PY{c+c1}{\PYZsh{} We start by defining the encoder and decoder components.}


\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}

\PY{k}{class} \PY{n+nc}{VAE}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAE}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)} 
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  

       
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{h1} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21}\PY{p}{(}\PY{n}{h1}\PY{p}{)}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22}\PY{p}{(}\PY{n}{h1}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{o}{*}\PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{k}{return} \PY{n}{mu} \PY{o}{+} \PY{n}{eps}\PY{o}{*}\PY{n}{std}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{h3} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4}\PY{p}{(}\PY{n}{h3}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}
        \PY{n}{z} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
    \PY{n}{BCE} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} KL散度}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD}

\PY{c+c1}{\PYZsh{} 创建模型实例}
\PY{n}{model} \PY{o}{=} \PY{n}{VAE}\PY{p}{(}\PY{p}{)}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{from} \PY{n+nn}{torchvision} \PY{k+kn}{import} \PY{n}{datasets}\PY{p}{,} \PY{n}{transforms}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{DataLoader}
\PY{k+kn}{from} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k+kn}{import} \PY{n}{save\PYZus{}image}


\PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{n}{transform} \PY{o}{=} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
    \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,} 
\PY{p}{]}\PY{p}{)}

\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{test\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{test\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}


\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{64}

\PY{n}{data\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{dataset}\PY{o}{=}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{k}{class} \PY{n+nc}{VAE}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAE}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
       
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  

       
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{h1} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21}\PY{p}{(}\PY{n}{h1}\PY{p}{)}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22}\PY{p}{(}\PY{n}{h1}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{k}{return} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{h3} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4}\PY{p}{(}\PY{n}{h3}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 确保输出在 [0, 1]}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}
        \PY{n}{z} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}


\PY{n}{model} \PY{o}{=} \PY{n}{VAE}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
   
    \PY{n}{BCE} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD}

\PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{batch\PYZus{}idx}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{)}\PY{p}{:}
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
        \PY{k}{if} \PY{n}{batch\PYZus{}idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ [}\PY{l+s+si}{\PYZob{}}\PY{n}{batch\PYZus{}idx}\PY{+w}{ }\PY{o}{*}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.6f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{====\PYZgt{} Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ Average loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{test}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
    \PY{n}{test\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{test\PYZus{}loader}\PY{p}{)}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{data}\PY{p}{)}
            \PY{n}{test\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}

            \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{n} \PY{o}{=} \PY{n+nb}{min}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}
                \PY{n}{comparison} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{n}{n}\PY{p}{]}\PY{p}{,} \PY{n}{recon\PYZus{}batch}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{n}{n}\PY{p}{]}\PY{p}{]}\PY{p}{)}
                \PY{n}{save\PYZus{}image}\PY{p}{(}\PY{n}{comparison}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reconstruction\PYZus{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{epoch}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nrow}\PY{o}{=}\PY{n}{n}\PY{p}{)}

    \PY{n}{test\PYZus{}loss} \PY{o}{/}\PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{====\PYZgt{} Test set loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{test\PYZus{}loss}\PY{l+s+si}{:}\PY{l+s+s1}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n}{train}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
    \PY{n}{test}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train Epoch: 1 [0/60000]        Loss: 551.919250
Train Epoch: 1 [6400/60000]     Loss: 182.184097
Train Epoch: 1 [12800/60000]    Loss: 160.768753
Train Epoch: 1 [19200/60000]    Loss: 146.986557
Train Epoch: 1 [25600/60000]    Loss: 134.226578
Train Epoch: 1 [32000/60000]    Loss: 127.335480
Train Epoch: 1 [38400/60000]    Loss: 132.698273
Train Epoch: 1 [44800/60000]    Loss: 126.019135
Train Epoch: 1 [51200/60000]    Loss: 123.994339
Train Epoch: 1 [57600/60000]    Loss: 120.055176
====> Epoch: 1 Average loss: 146.4137
====> Test set loss: 119.4359
Train Epoch: 2 [0/60000]        Loss: 117.034378
Train Epoch: 2 [6400/60000]     Loss: 114.082657
Train Epoch: 2 [12800/60000]    Loss: 111.612244
Train Epoch: 2 [19200/60000]    Loss: 116.158981
Train Epoch: 2 [25600/60000]    Loss: 117.521660
Train Epoch: 2 [32000/60000]    Loss: 117.935181
Train Epoch: 2 [38400/60000]    Loss: 114.167076
Train Epoch: 2 [44800/60000]    Loss: 113.666748
Train Epoch: 2 [51200/60000]    Loss: 113.663788
Train Epoch: 2 [57600/60000]    Loss: 111.847023
====> Epoch: 2 Average loss: 115.4172
====> Test set loss: 111.8258
Train Epoch: 3 [0/60000]        Loss: 113.436684
Train Epoch: 3 [6400/60000]     Loss: 109.977821
Train Epoch: 3 [12800/60000]    Loss: 116.638069
Train Epoch: 3 [19200/60000]    Loss: 112.055923
Train Epoch: 3 [25600/60000]    Loss: 111.278572
Train Epoch: 3 [32000/60000]    Loss: 107.828056
Train Epoch: 3 [38400/60000]    Loss: 101.563728
Train Epoch: 3 [44800/60000]    Loss: 111.164284
Train Epoch: 3 [51200/60000]    Loss: 115.798515
Train Epoch: 3 [57600/60000]    Loss: 110.774406
====> Epoch: 3 Average loss: 111.1608
====> Test set loss: 109.1689
Train Epoch: 4 [0/60000]        Loss: 104.054428
Train Epoch: 4 [6400/60000]     Loss: 113.087830
Train Epoch: 4 [12800/60000]    Loss: 106.848816
Train Epoch: 4 [19200/60000]    Loss: 105.746925
Train Epoch: 4 [25600/60000]    Loss: 111.881042
Train Epoch: 4 [32000/60000]    Loss: 116.088043
Train Epoch: 4 [38400/60000]    Loss: 115.689270
Train Epoch: 4 [44800/60000]    Loss: 107.331833
Train Epoch: 4 [51200/60000]    Loss: 105.795410
Train Epoch: 4 [57600/60000]    Loss: 108.316017
====> Epoch: 4 Average loss: 109.1728
====> Test set loss: 107.8859
Train Epoch: 5 [0/60000]        Loss: 105.699699
Train Epoch: 5 [6400/60000]     Loss: 109.980339
Train Epoch: 5 [12800/60000]    Loss: 111.590363
Train Epoch: 5 [19200/60000]    Loss: 108.682861
Train Epoch: 5 [25600/60000]    Loss: 111.743774
Train Epoch: 5 [32000/60000]    Loss: 110.864693
Train Epoch: 5 [38400/60000]    Loss: 106.427513
Train Epoch: 5 [44800/60000]    Loss: 107.434837
Train Epoch: 5 [51200/60000]    Loss: 111.152748
Train Epoch: 5 [57600/60000]    Loss: 105.850449
====> Epoch: 5 Average loss: 108.0194
====> Test set loss: 107.1133
Train Epoch: 6 [0/60000]        Loss: 103.628647
Train Epoch: 6 [6400/60000]     Loss: 103.281281
Train Epoch: 6 [12800/60000]    Loss: 105.837486
Train Epoch: 6 [19200/60000]    Loss: 108.514496
Train Epoch: 6 [25600/60000]    Loss: 106.206909
Train Epoch: 6 [32000/60000]    Loss: 107.776558
Train Epoch: 6 [38400/60000]    Loss: 103.632133
Train Epoch: 6 [44800/60000]    Loss: 107.543137
Train Epoch: 6 [51200/60000]    Loss: 105.774567
Train Epoch: 6 [57600/60000]    Loss: 104.187729
====> Epoch: 6 Average loss: 107.1999
====> Test set loss: 106.6534
Train Epoch: 7 [0/60000]        Loss: 101.738907
Train Epoch: 7 [6400/60000]     Loss: 106.827431
Train Epoch: 7 [12800/60000]    Loss: 108.391525
Train Epoch: 7 [19200/60000]    Loss: 105.876915
Train Epoch: 7 [25600/60000]    Loss: 109.297836
Train Epoch: 7 [32000/60000]    Loss: 110.194351
Train Epoch: 7 [38400/60000]    Loss: 108.250587
Train Epoch: 7 [44800/60000]    Loss: 100.728836
Train Epoch: 7 [51200/60000]    Loss: 111.466003
Train Epoch: 7 [57600/60000]    Loss: 102.639236
====> Epoch: 7 Average loss: 106.6182
====> Test set loss: 106.1118
Train Epoch: 8 [0/60000]        Loss: 109.441437
Train Epoch: 8 [6400/60000]     Loss: 108.864639
Train Epoch: 8 [12800/60000]    Loss: 106.313835
Train Epoch: 8 [19200/60000]    Loss: 107.764008
Train Epoch: 8 [25600/60000]    Loss: 105.121384
Train Epoch: 8 [32000/60000]    Loss: 100.524666
Train Epoch: 8 [38400/60000]    Loss: 104.180557
Train Epoch: 8 [44800/60000]    Loss: 109.969673
Train Epoch: 8 [51200/60000]    Loss: 106.098488
Train Epoch: 8 [57600/60000]    Loss: 104.331429
====> Epoch: 8 Average loss: 106.1324
====> Test set loss: 105.6353
Train Epoch: 9 [0/60000]        Loss: 103.444931
Train Epoch: 9 [6400/60000]     Loss: 104.519173
Train Epoch: 9 [12800/60000]    Loss: 111.503349
Train Epoch: 9 [19200/60000]    Loss: 101.432533
Train Epoch: 9 [25600/60000]    Loss: 110.798203
Train Epoch: 9 [32000/60000]    Loss: 108.348862
Train Epoch: 9 [38400/60000]    Loss: 106.928055
Train Epoch: 9 [44800/60000]    Loss: 97.816780
Train Epoch: 9 [51200/60000]    Loss: 108.421631
Train Epoch: 9 [57600/60000]    Loss: 100.129257
====> Epoch: 9 Average loss: 105.7254
====> Test set loss: 105.3206
Train Epoch: 10 [0/60000]       Loss: 103.752029
Train Epoch: 10 [6400/60000]    Loss: 108.575836
Train Epoch: 10 [12800/60000]   Loss: 106.196259
Train Epoch: 10 [19200/60000]   Loss: 106.441734
Train Epoch: 10 [25600/60000]   Loss: 107.783966
Train Epoch: 10 [32000/60000]   Loss: 106.886963
Train Epoch: 10 [38400/60000]   Loss: 104.403137
Train Epoch: 10 [44800/60000]   Loss: 112.906784
Train Epoch: 10 [51200/60000]   Loss: 108.084442
Train Epoch: 10 [57600/60000]   Loss: 104.700577
====> Epoch: 10 Average loss: 105.3731
====> Test set loss: 104.9960
    \end{Verbatim}

    During the training process, we observed the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Gradual Decrease in Loss}: As training progresses, both the
  training loss and test loss decrease steadily. This indicates that the
  VAE model is effectively learning the data distribution, and its
  reconstruction ability is improving over time.
\item
  \textbf{Convergence Speed}: In the initial epochs, the loss decreases
  rapidly and then gradually stabilizes. This is a common pattern in
  model training, indicating that the model is progressively converging.
\item
  \textbf{Final Loss Values}: By the 10th epoch, the average loss on the
  training set is 105.62, while the average loss on the test set is
  104.96. These values suggest that the model has reached a relatively
  stable state but may still have room for further optimization.
\item
  \textbf{Performance}: Although the model continues to improve, the
  rate of loss reduction slows down in the later epochs, indicating that
  the current model architecture might have some limitations in further
  reducing the loss on the MNIST dataset. This could be related to the
  latent space dimensions or the design of the encoder and decoder.
\end{enumerate}

    \subsubsection{Planar Flows}\label{planar-flows}

We are introducing Planar Flows into the VAE model to enhance the
flexibility and expressiveness of the latent space.

\textbf{What are Planar Flows?} Planar Flows are a simple yet effective
type of Normalizing Flow. They apply a series of invertible linear
transformations to the latent variables, making the latent space
distribution more complex and better at capturing data distribution.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}

\PY{k}{class} \PY{n+nc}{PlanarFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{PlanarFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} z shape: (batch\PYZus{}size, latent\PYZus{}dim)}
        \PY{n}{linear} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b}  \PY{c+c1}{\PYZsh{} shape: (batch\PYZus{}size)}
        \PY{n}{activation} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{linear}\PY{p}{)}  \PY{c+c1}{\PYZsh{} shape: (batch\PYZus{}size)}
        
        \PY{c+c1}{\PYZsh{} Add a dimension to u to enable broadcasting, i.e., (latent\PYZus{}dim,) \PYZhy{}\PYZgt{} (1, latent\PYZus{}dim)}
        \PY{n}{z\PYZus{}new} \PY{o}{=} \PY{n}{z} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u} \PY{o}{*} \PY{n}{activation}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} shape: (batch\PYZus{}size, latent\PYZus{}dim)}
        
        \PY{c+c1}{\PYZsh{} Compute psi, which will have shape (batch\PYZus{}size, latent\PYZus{}dim)}
        \PY{n}{psi} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{linear}\PY{p}{)}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}  \PY{c+c1}{\PYZsh{} shape: (batch\PYZus{}size, latent\PYZus{}dim)}
        
        \PY{c+c1}{\PYZsh{} Compute the determinant of the Jacobian, shape: (batch\PYZus{}size,)}
        \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{psi}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{return} \PY{n}{z\PYZus{}new}\PY{p}{,} \PY{n}{det\PYZus{}jacobian}


        
        
        
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{VAEWithFlows}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEWithFlows}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{latent\PYZus{}dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}flows} \PY{o}{=} \PY{n}{num\PYZus{}flows}
        
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  

       
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Planar Flows}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{PlanarFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}flows}\PY{p}{)}\PY{p}{]}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{h1} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21}\PY{p}{(}\PY{n}{h1}\PY{p}{)}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22}\PY{p}{(}\PY{n}{h1}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{n}{z0} \PY{o}{=} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}
        
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{z} \PY{o}{=} \PY{n}{z0}
        
      
        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{det\PYZus{}jacobian} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}
        
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{h3} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4}\PY{p}{(}\PY{n}{h3}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{loss\PYZus{}function\PYZus{}with\PYZus{}flows}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{p}{:}
    \PY{n}{BCE} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    
   
    \PY{n}{KLD} \PY{o}{=} \PY{n}{KLD} \PY{o}{\PYZhy{}} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{VAEWithFlows}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{batch\PYZus{}idx}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{)}\PY{p}{:}
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}function\PYZus{}with\PYZus{}flows}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
        \PY{k}{if} \PY{n}{batch\PYZus{}idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ [}\PY{l+s+si}{\PYZob{}}\PY{n}{batch\PYZus{}idx}\PY{+w}{ }\PY{o}{*}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{]}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.6f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{====\PYZgt{} Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ Average loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{test}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
    \PY{n}{test\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{test\PYZus{}loader}\PY{p}{)}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{data}\PY{p}{)}
            \PY{n}{test\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss\PYZus{}function\PYZus{}with\PYZus{}flows}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}

            \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{n} \PY{o}{=} \PY{n+nb}{min}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}
                \PY{n}{comparison} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{n}{n}\PY{p}{]}\PY{p}{,} \PY{n}{recon\PYZus{}batch}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{n}{n}\PY{p}{]}\PY{p}{]}\PY{p}{)}
                \PY{n}{save\PYZus{}image}\PY{p}{(}\PY{n}{comparison}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reconstruction\PYZus{}with\PYZus{}flows\PYZus{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{epoch}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nrow}\PY{o}{=}\PY{n}{n}\PY{p}{)}

    \PY{n}{test\PYZus{}loss} \PY{o}{/}\PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{====\PYZgt{} Test set loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{test\PYZus{}loss}\PY{l+s+si}{:}\PY{l+s+s1}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n}{train}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
    \PY{n}{test}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train Epoch: 1 [0/60000]        Loss: 556.946838
Train Epoch: 1 [6400/60000]     Loss: 196.831177
Train Epoch: 1 [12800/60000]    Loss: 162.402649
Train Epoch: 1 [19200/60000]    Loss: 153.010284
Train Epoch: 1 [25600/60000]    Loss: 142.892868
Train Epoch: 1 [32000/60000]    Loss: 133.016876
Train Epoch: 1 [38400/60000]    Loss: 129.801010
Train Epoch: 1 [44800/60000]    Loss: 126.503456
Train Epoch: 1 [51200/60000]    Loss: 131.550217
Train Epoch: 1 [57600/60000]    Loss: 127.067749
====> Epoch: 1 Average loss: 150.5395
====> Test set loss: 123.3879
Train Epoch: 2 [0/60000]        Loss: 117.438263
Train Epoch: 2 [6400/60000]     Loss: 126.851456
Train Epoch: 2 [12800/60000]    Loss: 124.902420
Train Epoch: 2 [19200/60000]    Loss: 122.360931
Train Epoch: 2 [25600/60000]    Loss: 117.555168
Train Epoch: 2 [32000/60000]    Loss: 116.779045
Train Epoch: 2 [38400/60000]    Loss: 110.066292
Train Epoch: 2 [44800/60000]    Loss: 113.392258
Train Epoch: 2 [51200/60000]    Loss: 117.371048
Train Epoch: 2 [57600/60000]    Loss: 117.026505
====> Epoch: 2 Average loss: 119.2312
====> Test set loss: 114.0699
Train Epoch: 3 [0/60000]        Loss: 115.382057
Train Epoch: 3 [6400/60000]     Loss: 108.025475
Train Epoch: 3 [12800/60000]    Loss: 116.623398
Train Epoch: 3 [19200/60000]    Loss: 113.990570
Train Epoch: 3 [25600/60000]    Loss: 110.621033
Train Epoch: 3 [32000/60000]    Loss: 112.123047
Train Epoch: 3 [38400/60000]    Loss: 106.969559
Train Epoch: 3 [44800/60000]    Loss: 110.406799
Train Epoch: 3 [51200/60000]    Loss: 119.068817
Train Epoch: 3 [57600/60000]    Loss: 109.565742
====> Epoch: 3 Average loss: 112.8788
====> Test set loss: 110.0252
Train Epoch: 4 [0/60000]        Loss: 107.801819
Train Epoch: 4 [6400/60000]     Loss: 106.599533
Train Epoch: 4 [12800/60000]    Loss: 113.195885
Train Epoch: 4 [19200/60000]    Loss: 109.285591
Train Epoch: 4 [25600/60000]    Loss: 112.686188
Train Epoch: 4 [32000/60000]    Loss: 103.254128
Train Epoch: 4 [38400/60000]    Loss: 107.956390
Train Epoch: 4 [44800/60000]    Loss: 114.667099
Train Epoch: 4 [51200/60000]    Loss: 104.335541
Train Epoch: 4 [57600/60000]    Loss: 102.767899
====> Epoch: 4 Average loss: 109.3005
====> Test set loss: 107.2092
Train Epoch: 5 [0/60000]        Loss: 107.941963
Train Epoch: 5 [6400/60000]     Loss: 106.589745
Train Epoch: 5 [12800/60000]    Loss: 107.095276
Train Epoch: 5 [19200/60000]    Loss: 107.577019
Train Epoch: 5 [25600/60000]    Loss: 111.295303
Train Epoch: 5 [32000/60000]    Loss: 110.598343
Train Epoch: 5 [38400/60000]    Loss: 108.069534
Train Epoch: 5 [44800/60000]    Loss: 104.231277
Train Epoch: 5 [51200/60000]    Loss: 106.946075
Train Epoch: 5 [57600/60000]    Loss: 103.621399
====> Epoch: 5 Average loss: 107.1554
====> Test set loss: 105.7634
Train Epoch: 6 [0/60000]        Loss: 106.768494
Train Epoch: 6 [6400/60000]     Loss: 109.946899
Train Epoch: 6 [12800/60000]    Loss: 105.051102
Train Epoch: 6 [19200/60000]    Loss: 104.902290
Train Epoch: 6 [25600/60000]    Loss: 99.521812
Train Epoch: 6 [32000/60000]    Loss: 110.650818
Train Epoch: 6 [38400/60000]    Loss: 108.857925
Train Epoch: 6 [44800/60000]    Loss: 102.431229
Train Epoch: 6 [51200/60000]    Loss: 100.170464
Train Epoch: 6 [57600/60000]    Loss: 104.804916
====> Epoch: 6 Average loss: 105.7350
====> Test set loss: 104.4674
Train Epoch: 7 [0/60000]        Loss: 104.887192
Train Epoch: 7 [6400/60000]     Loss: 100.655975
Train Epoch: 7 [12800/60000]    Loss: 98.495056
Train Epoch: 7 [19200/60000]    Loss: 106.359848
Train Epoch: 7 [25600/60000]    Loss: 104.719810
Train Epoch: 7 [32000/60000]    Loss: 104.474014
Train Epoch: 7 [38400/60000]    Loss: 109.240303
Train Epoch: 7 [44800/60000]    Loss: 101.162491
Train Epoch: 7 [51200/60000]    Loss: 109.285683
Train Epoch: 7 [57600/60000]    Loss: 104.148903
====> Epoch: 7 Average loss: 104.7413
====> Test set loss: 103.8603
Train Epoch: 8 [0/60000]        Loss: 109.676201
Train Epoch: 8 [6400/60000]     Loss: 104.920609
Train Epoch: 8 [12800/60000]    Loss: 98.264046
Train Epoch: 8 [19200/60000]    Loss: 105.062134
Train Epoch: 8 [25600/60000]    Loss: 100.268745
Train Epoch: 8 [32000/60000]    Loss: 97.970490
Train Epoch: 8 [38400/60000]    Loss: 103.641556
Train Epoch: 8 [44800/60000]    Loss: 106.670769
Train Epoch: 8 [51200/60000]    Loss: 107.257339
Train Epoch: 8 [57600/60000]    Loss: 102.967819
====> Epoch: 8 Average loss: 103.9211
====> Test set loss: 103.0108
Train Epoch: 9 [0/60000]        Loss: 109.330444
Train Epoch: 9 [6400/60000]     Loss: 100.687614
Train Epoch: 9 [12800/60000]    Loss: 106.326492
Train Epoch: 9 [19200/60000]    Loss: 106.605026
Train Epoch: 9 [25600/60000]    Loss: 97.284012
Train Epoch: 9 [32000/60000]    Loss: 104.188423
Train Epoch: 9 [38400/60000]    Loss: 102.945663
Train Epoch: 9 [44800/60000]    Loss: 106.897697
Train Epoch: 9 [51200/60000]    Loss: 104.220490
Train Epoch: 9 [57600/60000]    Loss: 102.940277
====> Epoch: 9 Average loss: 103.2698
====> Test set loss: 102.4828
Train Epoch: 10 [0/60000]       Loss: 104.927757
Train Epoch: 10 [6400/60000]    Loss: 96.868744
Train Epoch: 10 [12800/60000]   Loss: 100.284195
Train Epoch: 10 [19200/60000]   Loss: 101.415451
Train Epoch: 10 [25600/60000]   Loss: 101.656357
Train Epoch: 10 [32000/60000]   Loss: 101.800552
Train Epoch: 10 [38400/60000]   Loss: 100.305908
Train Epoch: 10 [44800/60000]   Loss: 100.187973
Train Epoch: 10 [51200/60000]   Loss: 101.634964
Train Epoch: 10 [57600/60000]   Loss: 98.215576
====> Epoch: 10 Average loss: 102.7517
====> Test set loss: 102.1331
    \end{Verbatim}

    \subsubsection{Key Points on Introducing Normalizing
Flows}\label{key-points-on-introducing-normalizing-flows}

\begin{itemize}
\item
  \textbf{Flow Quantity and Type}: Too few flows may not enhance the
  model; too many can cause instability. Planar Flows might be too
  simple---consider using RealNVP or Glow.
\item
  \textbf{Optimization Issues}: Numerical instability from Jacobian
  calculations; requires careful tuning of learning rate and parameter
  initialization.
\item
  \textbf{Latent Space and Complexity}: High-dimensional latent space
  increases model complexity, making optimization difficult.
\item
  \textbf{Dataset Simplicity}: On simple datasets like MNIST, Planar
  Flows may show limited benefits.
\end{itemize}

\subsubsection{Recommendations}\label{recommendations}

\begin{itemize}
\tightlist
\item
  \textbf{Adjust Flow Layers}: Use 2-5 flow layers and monitor results.
\item
  \textbf{Explore Different Flows}: Try RealNVP or Radial Flows.
\item
  \textbf{Tune Hyperparameters}: Lower the learning rate and adjust
  initialization.
\item
  \textbf{Debug Incrementally}: Add flows step-by-step and visualize
  latent space changes.
\end{itemize}

\subsubsection{Example Adjustment:}\label{example-adjustment}

Reduce flow layers to 2 and decrease the learning rate to \texttt{1e-4}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{VAEWithFlows}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEWithFlows}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{latent\PYZus{}dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}flows} \PY{o}{=} \PY{n}{num\PYZus{}flows}
        
      
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  

     
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Planar Flows}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{PlanarFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}flows}\PY{p}{)}\PY{p}{]}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{h1} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21}\PY{p}{(}\PY{n}{h1}\PY{p}{)}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22}\PY{p}{(}\PY{n}{h1}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{n}{z0} \PY{o}{=} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}
        
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{z} \PY{o}{=} \PY{n}{z0}
        
     
        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{det\PYZus{}jacobian} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}
        
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{h3} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4}\PY{p}{(}\PY{n}{h3}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    

\PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}


\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n}{train}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
    \PY{n}{test}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}

    
    
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train Epoch: 1 [0/60000]        Loss: 108.970695
Train Epoch: 1 [6400/60000]     Loss: 96.966873
Train Epoch: 1 [12800/60000]    Loss: 99.417160
Train Epoch: 1 [19200/60000]    Loss: 108.783051
Train Epoch: 1 [25600/60000]    Loss: 101.173264
Train Epoch: 1 [32000/60000]    Loss: 96.929878
Train Epoch: 1 [38400/60000]    Loss: 100.307625
Train Epoch: 1 [44800/60000]    Loss: 104.952232
Train Epoch: 1 [51200/60000]    Loss: 102.790001
Train Epoch: 1 [57600/60000]    Loss: 100.911514
====> Epoch: 1 Average loss: 100.8352
====> Test set loss: 100.4292
Train Epoch: 2 [0/60000]        Loss: 99.368614
Train Epoch: 2 [6400/60000]     Loss: 101.848206
Train Epoch: 2 [12800/60000]    Loss: 102.184288
Train Epoch: 2 [19200/60000]    Loss: 103.012375
Train Epoch: 2 [25600/60000]    Loss: 99.054466
Train Epoch: 2 [32000/60000]    Loss: 96.746216
Train Epoch: 2 [38400/60000]    Loss: 100.770004
Train Epoch: 2 [44800/60000]    Loss: 97.793381
Train Epoch: 2 [51200/60000]    Loss: 98.234146
Train Epoch: 2 [57600/60000]    Loss: 96.082634
====> Epoch: 2 Average loss: 100.5479
====> Test set loss: 100.2379
Train Epoch: 3 [0/60000]        Loss: 101.912964
Train Epoch: 3 [6400/60000]     Loss: 99.520050
Train Epoch: 3 [12800/60000]    Loss: 94.996780
Train Epoch: 3 [19200/60000]    Loss: 101.873367
Train Epoch: 3 [25600/60000]    Loss: 102.823135
Train Epoch: 3 [32000/60000]    Loss: 104.092651
Train Epoch: 3 [38400/60000]    Loss: 95.973297
Train Epoch: 3 [44800/60000]    Loss: 97.674530
Train Epoch: 3 [51200/60000]    Loss: 99.274628
Train Epoch: 3 [57600/60000]    Loss: 98.477104
====> Epoch: 3 Average loss: 100.4291
====> Test set loss: 100.2028
Train Epoch: 4 [0/60000]        Loss: 100.542000
Train Epoch: 4 [6400/60000]     Loss: 100.172150
Train Epoch: 4 [12800/60000]    Loss: 105.120476
Train Epoch: 4 [19200/60000]    Loss: 99.481110
Train Epoch: 4 [25600/60000]    Loss: 98.112511
Train Epoch: 4 [32000/60000]    Loss: 99.576424
Train Epoch: 4 [38400/60000]    Loss: 92.990288
Train Epoch: 4 [44800/60000]    Loss: 97.850029
Train Epoch: 4 [51200/60000]    Loss: 98.976753
Train Epoch: 4 [57600/60000]    Loss: 106.096382
====> Epoch: 4 Average loss: 100.3881
====> Test set loss: 100.1308
Train Epoch: 5 [0/60000]        Loss: 92.071899
Train Epoch: 5 [6400/60000]     Loss: 95.853951
Train Epoch: 5 [12800/60000]    Loss: 97.642525
Train Epoch: 5 [19200/60000]    Loss: 97.680038
Train Epoch: 5 [25600/60000]    Loss: 99.213478
Train Epoch: 5 [32000/60000]    Loss: 96.312363
Train Epoch: 5 [38400/60000]    Loss: 103.351585
Train Epoch: 5 [44800/60000]    Loss: 96.226082
Train Epoch: 5 [51200/60000]    Loss: 98.462624
Train Epoch: 5 [57600/60000]    Loss: 102.726624
====> Epoch: 5 Average loss: 100.2933
====> Test set loss: 100.0404
Train Epoch: 6 [0/60000]        Loss: 103.948547
Train Epoch: 6 [6400/60000]     Loss: 100.949104
Train Epoch: 6 [12800/60000]    Loss: 99.942261
Train Epoch: 6 [19200/60000]    Loss: 99.857437
Train Epoch: 6 [25600/60000]    Loss: 99.641685
Train Epoch: 6 [32000/60000]    Loss: 104.971573
Train Epoch: 6 [38400/60000]    Loss: 96.607880
Train Epoch: 6 [44800/60000]    Loss: 101.415695
Train Epoch: 6 [51200/60000]    Loss: 99.901917
Train Epoch: 6 [57600/60000]    Loss: 109.574684
====> Epoch: 6 Average loss: 100.2139
====> Test set loss: 100.0030
Train Epoch: 7 [0/60000]        Loss: 98.976929
Train Epoch: 7 [6400/60000]     Loss: 101.972466
Train Epoch: 7 [12800/60000]    Loss: 98.293327
Train Epoch: 7 [19200/60000]    Loss: 98.376633
Train Epoch: 7 [25600/60000]    Loss: 100.403687
Train Epoch: 7 [32000/60000]    Loss: 103.103516
Train Epoch: 7 [38400/60000]    Loss: 99.818733
Train Epoch: 7 [44800/60000]    Loss: 101.364365
Train Epoch: 7 [51200/60000]    Loss: 97.915863
Train Epoch: 7 [57600/60000]    Loss: 98.247116
====> Epoch: 7 Average loss: 100.1447
====> Test set loss: 100.0205
Train Epoch: 8 [0/60000]        Loss: 102.192108
Train Epoch: 8 [6400/60000]     Loss: 100.247055
Train Epoch: 8 [12800/60000]    Loss: 100.321686
Train Epoch: 8 [19200/60000]    Loss: 101.854553
Train Epoch: 8 [25600/60000]    Loss: 101.253853
Train Epoch: 8 [32000/60000]    Loss: 97.376038
Train Epoch: 8 [38400/60000]    Loss: 102.805428
Train Epoch: 8 [44800/60000]    Loss: 103.457466
Train Epoch: 8 [51200/60000]    Loss: 96.180656
Train Epoch: 8 [57600/60000]    Loss: 94.424225
====> Epoch: 8 Average loss: 100.0897
====> Test set loss: 99.9165
Train Epoch: 9 [0/60000]        Loss: 94.239388
Train Epoch: 9 [6400/60000]     Loss: 101.788406
Train Epoch: 9 [12800/60000]    Loss: 100.479416
Train Epoch: 9 [19200/60000]    Loss: 104.938370
Train Epoch: 9 [25600/60000]    Loss: 104.765564
Train Epoch: 9 [32000/60000]    Loss: 97.631172
Train Epoch: 9 [38400/60000]    Loss: 102.504517
Train Epoch: 9 [44800/60000]    Loss: 102.987328
Train Epoch: 9 [51200/60000]    Loss: 99.074532
Train Epoch: 9 [57600/60000]    Loss: 96.581345
====> Epoch: 9 Average loss: 100.0531
====> Test set loss: 99.8900
Train Epoch: 10 [0/60000]       Loss: 100.069214
Train Epoch: 10 [6400/60000]    Loss: 101.746597
Train Epoch: 10 [12800/60000]   Loss: 103.116196
Train Epoch: 10 [19200/60000]   Loss: 109.222870
Train Epoch: 10 [25600/60000]   Loss: 98.201706
Train Epoch: 10 [32000/60000]   Loss: 100.829651
Train Epoch: 10 [38400/60000]   Loss: 102.449371
Train Epoch: 10 [44800/60000]   Loss: 103.482803
Train Epoch: 10 [51200/60000]   Loss: 96.879311
Train Epoch: 10 [57600/60000]   Loss: 99.116119
====> Epoch: 10 Average loss: 99.9592
====> Test set loss: 99.7775
    \end{Verbatim}

    \subsubsection{RealNVP (Real-valued Non-Volume Preserving) Flow: Key
Points}\label{realnvp-real-valued-non-volume-preserving-flow-key-points}

\textbf{RealNVP} is a type of Normalizing Flow that allows for complex,
non-linear transformations of data while keeping the computation of the
Jacobian determinant tractable. It is particularly useful in generative
modeling, where the goal is to transform a simple distribution (like a
Gaussian) into a complex data distribution.

\paragraph{Key Concepts:}\label{key-concepts}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Affine Coupling Layers}:

  \begin{itemize}
  \tightlist
  \item
    RealNVP utilizes \emph{affine coupling layers}, which split the
    input data into two parts: one part remains unchanged, and the other
    part is transformed conditionally based on the unchanged part. This
    transformation typically involves scaling and translation
    operations.
  \end{itemize}
\item
  \textbf{Invertibility}:

  \begin{itemize}
  \tightlist
  \item
    The transformation is designed to be easily invertible. This means
    that given the output, the input can be exactly recovered, which is
    crucial for both density estimation and sampling.
  \end{itemize}
\item
  \textbf{Efficient Jacobian Computation}:

  \begin{itemize}
  \tightlist
  \item
    A major advantage of RealNVP is that the Jacobian of the
    transformation is triangular, making its determinant easy to compute
    as the product of the diagonal elements. This efficiency is key to
    applying RealNVP in high-dimensional settings.
  \end{itemize}
\item
  \textbf{Complexity}:

  \begin{itemize}
  \tightlist
  \item
    Despite the simplicity of each individual transformation, stacking
    multiple affine coupling layers allows RealNVP to model very
    complex, high-dimensional distributions.
  \end{itemize}
\item
  \textbf{Application in Generative Models}:

  \begin{itemize}
  \tightlist
  \item
    RealNVP is often used in Variational Autoencoders (VAEs) and other
    generative models to improve the expressiveness of the latent space,
    leading to better quality in generated samples.
  \end{itemize}
\end{enumerate}

RealNVP's ability to balance expressiveness with computational
efficiency makes it a powerful tool in modern generative modeling.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{RealNVPFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{RealNVPFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input\PYZus{}dim} \PY{o}{=} \PY{n}{input\PYZus{}dim}

        \PY{c+c1}{\PYZsh{} 创建条件变换函数，s(x) 和 t(x)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{s} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}
        \PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}\PY{p}{)}
        \PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{z1}\PY{p}{,} \PY{n}{z2} \PY{o}{=} \PY{n}{z}\PY{o}{.}\PY{n}{chunk}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{s} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{s}\PY{p}{(}\PY{n}{z2}\PY{p}{)}
        \PY{n}{t} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t}\PY{p}{(}\PY{n}{z2}\PY{p}{)}
        \PY{n}{z1} \PY{o}{=} \PY{n}{z1} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{s}\PY{p}{)} \PY{o}{+} \PY{n}{t}
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{s}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{[}\PY{n}{z1}\PY{p}{,} \PY{n}{z2}\PY{p}{]}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    
\PY{k}{class} \PY{n+nc}{VAEWithRealNVP}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEWithRealNVP}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{latent\PYZus{}dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}flows} \PY{o}{=} \PY{n}{num\PYZus{}flows}

       
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  

     
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} RealNVP Flows}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{RealNVPFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}flows}\PY{p}{)}\PY{p}{]}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{h1} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21}\PY{p}{(}\PY{n}{h1}\PY{p}{)}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22}\PY{p}{(}\PY{n}{h1}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{n}{z0} \PY{o}{=} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}
        
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{z} \PY{o}{=} \PY{n}{z0}

        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{det\PYZus{}jacobian}

        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{h3} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4}\PY{p}{(}\PY{n}{h3}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}
    \PY{k}{def} \PY{n+nf}{generate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{VAEWithRealNVP}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}


\PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}


\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{21}\PY{p}{)}\PY{p}{:}
    \PY{n}{train}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
    \PY{n}{test}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train Epoch: 1 [0/60000]        Loss: 551.756714
Train Epoch: 1 [6400/60000]     Loss: 197.183884
Train Epoch: 1 [12800/60000]    Loss: 185.453918
Train Epoch: 1 [19200/60000]    Loss: 155.786942
Train Epoch: 1 [25600/60000]    Loss: 142.300949
Train Epoch: 1 [32000/60000]    Loss: 118.001137
Train Epoch: 1 [38400/60000]    Loss: 110.788528
Train Epoch: 1 [44800/60000]    Loss: 104.693710
Train Epoch: 1 [51200/60000]    Loss: 112.641640
Train Epoch: 1 [57600/60000]    Loss: 104.319725
====> Epoch: 1 Average loss: 153.4320
====> Test set loss: 101.0385
Train Epoch: 2 [0/60000]        Loss: 103.739388
Train Epoch: 2 [6400/60000]     Loss: 99.047760
Train Epoch: 2 [12800/60000]    Loss: 98.032654
Train Epoch: 2 [19200/60000]    Loss: 87.395500
Train Epoch: 2 [25600/60000]    Loss: 97.936234
Train Epoch: 2 [32000/60000]    Loss: 90.205414
Train Epoch: 2 [38400/60000]    Loss: 100.069115
Train Epoch: 2 [44800/60000]    Loss: 85.286331
Train Epoch: 2 [51200/60000]    Loss: 102.036392
Train Epoch: 2 [57600/60000]    Loss: 99.483765
====> Epoch: 2 Average loss: 95.7020
====> Test set loss: 90.0949
Train Epoch: 3 [0/60000]        Loss: 90.146347
Train Epoch: 3 [6400/60000]     Loss: 98.167053
Train Epoch: 3 [12800/60000]    Loss: 88.786018
Train Epoch: 3 [19200/60000]    Loss: 91.922897
Train Epoch: 3 [25600/60000]    Loss: 87.879845
Train Epoch: 3 [32000/60000]    Loss: 98.232658
Train Epoch: 3 [38400/60000]    Loss: 92.567497
Train Epoch: 3 [44800/60000]    Loss: 77.174553
Train Epoch: 3 [51200/60000]    Loss: 92.535332
Train Epoch: 3 [57600/60000]    Loss: 89.905716
====> Epoch: 3 Average loss: 88.1987
====> Test set loss: 85.1761
Train Epoch: 4 [0/60000]        Loss: 83.373878
Train Epoch: 4 [6400/60000]     Loss: 87.055344
Train Epoch: 4 [12800/60000]    Loss: 79.175407
Train Epoch: 4 [19200/60000]    Loss: 90.048676
Train Epoch: 4 [25600/60000]    Loss: 80.082336
Train Epoch: 4 [32000/60000]    Loss: 86.779060
Train Epoch: 4 [38400/60000]    Loss: 82.935371
Train Epoch: 4 [44800/60000]    Loss: 82.226067
Train Epoch: 4 [51200/60000]    Loss: 78.933868
Train Epoch: 4 [57600/60000]    Loss: 82.116127
====> Epoch: 4 Average loss: 84.3082
====> Test set loss: 82.1300
Train Epoch: 5 [0/60000]        Loss: 87.049187
Train Epoch: 5 [6400/60000]     Loss: 83.498672
Train Epoch: 5 [12800/60000]    Loss: 83.078346
Train Epoch: 5 [19200/60000]    Loss: 76.693062
Train Epoch: 5 [25600/60000]    Loss: 87.979401
Train Epoch: 5 [32000/60000]    Loss: 79.942703
Train Epoch: 5 [38400/60000]    Loss: 88.626625
Train Epoch: 5 [44800/60000]    Loss: 83.851570
Train Epoch: 5 [51200/60000]    Loss: 85.909439
Train Epoch: 5 [57600/60000]    Loss: 85.255554
====> Epoch: 5 Average loss: 81.7602
====> Test set loss: 79.9964
Train Epoch: 6 [0/60000]        Loss: 75.037903
Train Epoch: 6 [6400/60000]     Loss: 82.717148
Train Epoch: 6 [12800/60000]    Loss: 78.224106
Train Epoch: 6 [19200/60000]    Loss: 78.752655
Train Epoch: 6 [25600/60000]    Loss: 86.960312
Train Epoch: 6 [32000/60000]    Loss: 83.773796
Train Epoch: 6 [38400/60000]    Loss: 83.061050
Train Epoch: 6 [44800/60000]    Loss: 82.571426
Train Epoch: 6 [51200/60000]    Loss: 84.534698
Train Epoch: 6 [57600/60000]    Loss: 85.419586
====> Epoch: 6 Average loss: 79.9172
====> Test set loss: 78.7322
Train Epoch: 7 [0/60000]        Loss: 81.335571
Train Epoch: 7 [6400/60000]     Loss: 86.707253
Train Epoch: 7 [12800/60000]    Loss: 73.339455
Train Epoch: 7 [19200/60000]    Loss: 81.922371
Train Epoch: 7 [25600/60000]    Loss: 78.770020
Train Epoch: 7 [32000/60000]    Loss: 79.392929
Train Epoch: 7 [38400/60000]    Loss: 69.641815
Train Epoch: 7 [44800/60000]    Loss: 74.152260
Train Epoch: 7 [51200/60000]    Loss: 84.147385
Train Epoch: 7 [57600/60000]    Loss: 81.352310
====> Epoch: 7 Average loss: 78.5317
====> Test set loss: 77.3837
Train Epoch: 8 [0/60000]        Loss: 86.128189
Train Epoch: 8 [6400/60000]     Loss: 81.599869
Train Epoch: 8 [12800/60000]    Loss: 68.548401
Train Epoch: 8 [19200/60000]    Loss: 72.827957
Train Epoch: 8 [25600/60000]    Loss: 80.178284
Train Epoch: 8 [32000/60000]    Loss: 74.195251
Train Epoch: 8 [38400/60000]    Loss: 73.222862
Train Epoch: 8 [44800/60000]    Loss: 75.741219
Train Epoch: 8 [51200/60000]    Loss: 68.513596
Train Epoch: 8 [57600/60000]    Loss: 78.336708
====> Epoch: 8 Average loss: 77.4449
====> Test set loss: 76.6189
Train Epoch: 9 [0/60000]        Loss: 78.107712
Train Epoch: 9 [6400/60000]     Loss: 77.153732
Train Epoch: 9 [12800/60000]    Loss: 80.386993
Train Epoch: 9 [19200/60000]    Loss: 79.284439
Train Epoch: 9 [25600/60000]    Loss: 76.154205
Train Epoch: 9 [32000/60000]    Loss: 81.156052
Train Epoch: 9 [38400/60000]    Loss: 77.683403
Train Epoch: 9 [44800/60000]    Loss: 74.097809
Train Epoch: 9 [51200/60000]    Loss: 72.305847
Train Epoch: 9 [57600/60000]    Loss: 78.783981
====> Epoch: 9 Average loss: 76.4566
====> Test set loss: 75.6532
Train Epoch: 10 [0/60000]       Loss: 78.064987
Train Epoch: 10 [6400/60000]    Loss: 82.048218
Train Epoch: 10 [12800/60000]   Loss: 71.297211
Train Epoch: 10 [19200/60000]   Loss: 76.313980
Train Epoch: 10 [25600/60000]   Loss: 76.824059
Train Epoch: 10 [32000/60000]   Loss: 81.752075
Train Epoch: 10 [38400/60000]   Loss: 77.203346
Train Epoch: 10 [44800/60000]   Loss: 73.637474
Train Epoch: 10 [51200/60000]   Loss: 80.989220
Train Epoch: 10 [57600/60000]   Loss: 74.291817
====> Epoch: 10 Average loss: 75.7266
====> Test set loss: 75.1791
Train Epoch: 11 [0/60000]       Loss: 81.585602
Train Epoch: 11 [6400/60000]    Loss: 75.711540
Train Epoch: 11 [12800/60000]   Loss: 76.101776
Train Epoch: 11 [19200/60000]   Loss: 73.047882
Train Epoch: 11 [25600/60000]   Loss: 73.072433
Train Epoch: 11 [32000/60000]   Loss: 75.694496
Train Epoch: 11 [38400/60000]   Loss: 74.458862
Train Epoch: 11 [44800/60000]   Loss: 79.741867
Train Epoch: 11 [51200/60000]   Loss: 69.525116
Train Epoch: 11 [57600/60000]   Loss: 78.452942
====> Epoch: 11 Average loss: 75.0154
====> Test set loss: 74.3609
Train Epoch: 12 [0/60000]       Loss: 78.385925
Train Epoch: 12 [6400/60000]    Loss: 72.761597
Train Epoch: 12 [12800/60000]   Loss: 72.591393
Train Epoch: 12 [19200/60000]   Loss: 75.291664
Train Epoch: 12 [25600/60000]   Loss: 77.313446
Train Epoch: 12 [32000/60000]   Loss: 70.997513
Train Epoch: 12 [38400/60000]   Loss: 73.634369
Train Epoch: 12 [44800/60000]   Loss: 71.980301
Train Epoch: 12 [51200/60000]   Loss: 71.638618
Train Epoch: 12 [57600/60000]   Loss: 72.352081
====> Epoch: 12 Average loss: 74.3803
====> Test set loss: 73.9742
Train Epoch: 13 [0/60000]       Loss: 74.040451
Train Epoch: 13 [6400/60000]    Loss: 72.079208
Train Epoch: 13 [12800/60000]   Loss: 72.358231
Train Epoch: 13 [19200/60000]   Loss: 78.307556
Train Epoch: 13 [25600/60000]   Loss: 72.312889
Train Epoch: 13 [32000/60000]   Loss: 72.537346
Train Epoch: 13 [38400/60000]   Loss: 69.642830
Train Epoch: 13 [44800/60000]   Loss: 74.637100
Train Epoch: 13 [51200/60000]   Loss: 76.438431
Train Epoch: 13 [57600/60000]   Loss: 75.166145
====> Epoch: 13 Average loss: 73.8628
====> Test set loss: 73.5870
Train Epoch: 14 [0/60000]       Loss: 71.037666
Train Epoch: 14 [6400/60000]    Loss: 69.710197
Train Epoch: 14 [12800/60000]   Loss: 77.722633
Train Epoch: 14 [19200/60000]   Loss: 71.351677
Train Epoch: 14 [25600/60000]   Loss: 77.945671
Train Epoch: 14 [32000/60000]   Loss: 80.104820
Train Epoch: 14 [38400/60000]   Loss: 69.318024
Train Epoch: 14 [44800/60000]   Loss: 72.534485
Train Epoch: 14 [51200/60000]   Loss: 69.857033
Train Epoch: 14 [57600/60000]   Loss: 74.110733
====> Epoch: 14 Average loss: 73.3888
====> Test set loss: 73.0152
Train Epoch: 15 [0/60000]       Loss: 66.049034
Train Epoch: 15 [6400/60000]    Loss: 73.404701
Train Epoch: 15 [12800/60000]   Loss: 75.803108
Train Epoch: 15 [19200/60000]   Loss: 72.853638
Train Epoch: 15 [25600/60000]   Loss: 77.642159
Train Epoch: 15 [32000/60000]   Loss: 71.843613
Train Epoch: 15 [38400/60000]   Loss: 70.052444
Train Epoch: 15 [44800/60000]   Loss: 74.069122
Train Epoch: 15 [51200/60000]   Loss: 72.857605
Train Epoch: 15 [57600/60000]   Loss: 67.223396
====> Epoch: 15 Average loss: 72.9582
====> Test set loss: 72.6762
Train Epoch: 16 [0/60000]       Loss: 66.102554
Train Epoch: 16 [6400/60000]    Loss: 77.322372
Train Epoch: 16 [12800/60000]   Loss: 77.480843
Train Epoch: 16 [19200/60000]   Loss: 68.280106
Train Epoch: 16 [25600/60000]   Loss: 75.477455
Train Epoch: 16 [32000/60000]   Loss: 72.714272
Train Epoch: 16 [38400/60000]   Loss: 75.143394
Train Epoch: 16 [44800/60000]   Loss: 71.397766
Train Epoch: 16 [51200/60000]   Loss: 71.671265
Train Epoch: 16 [57600/60000]   Loss: 71.551392
====> Epoch: 16 Average loss: 72.5366
====> Test set loss: 72.2397
Train Epoch: 17 [0/60000]       Loss: 76.392326
Train Epoch: 17 [6400/60000]    Loss: 77.339188
Train Epoch: 17 [12800/60000]   Loss: 73.555244
Train Epoch: 17 [19200/60000]   Loss: 81.549316
Train Epoch: 17 [25600/60000]   Loss: 71.233780
Train Epoch: 17 [32000/60000]   Loss: 71.538918
Train Epoch: 17 [38400/60000]   Loss: 71.460487
Train Epoch: 17 [44800/60000]   Loss: 68.403320
Train Epoch: 17 [51200/60000]   Loss: 71.029716
Train Epoch: 17 [57600/60000]   Loss: 65.840134
====> Epoch: 17 Average loss: 72.1952
====> Test set loss: 71.9793
Train Epoch: 18 [0/60000]       Loss: 75.502213
Train Epoch: 18 [6400/60000]    Loss: 73.839310
Train Epoch: 18 [12800/60000]   Loss: 74.992119
Train Epoch: 18 [19200/60000]   Loss: 73.201019
Train Epoch: 18 [25600/60000]   Loss: 68.710732
Train Epoch: 18 [32000/60000]   Loss: 68.146294
Train Epoch: 18 [38400/60000]   Loss: 73.385796
Train Epoch: 18 [44800/60000]   Loss: 68.954475
Train Epoch: 18 [51200/60000]   Loss: 66.669319
Train Epoch: 18 [57600/60000]   Loss: 71.626976
====> Epoch: 18 Average loss: 71.8235
====> Test set loss: 71.5028
Train Epoch: 19 [0/60000]       Loss: 68.570160
Train Epoch: 19 [6400/60000]    Loss: 64.234276
Train Epoch: 19 [12800/60000]   Loss: 74.848740
Train Epoch: 19 [19200/60000]   Loss: 68.486122
Train Epoch: 19 [25600/60000]   Loss: 77.457397
Train Epoch: 19 [32000/60000]   Loss: 72.375473
Train Epoch: 19 [38400/60000]   Loss: 66.836563
Train Epoch: 19 [44800/60000]   Loss: 74.704353
Train Epoch: 19 [51200/60000]   Loss: 72.107086
Train Epoch: 19 [57600/60000]   Loss: 64.773590
====> Epoch: 19 Average loss: 71.4978
====> Test set loss: 71.5658
Train Epoch: 20 [0/60000]       Loss: 66.934860
Train Epoch: 20 [6400/60000]    Loss: 70.811432
Train Epoch: 20 [12800/60000]   Loss: 70.686981
Train Epoch: 20 [19200/60000]   Loss: 68.395294
Train Epoch: 20 [25600/60000]   Loss: 65.428879
Train Epoch: 20 [32000/60000]   Loss: 70.251091
Train Epoch: 20 [38400/60000]   Loss: 76.756287
Train Epoch: 20 [44800/60000]   Loss: 76.113640
Train Epoch: 20 [51200/60000]   Loss: 67.048058
Train Epoch: 20 [57600/60000]   Loss: 72.799965
====> Epoch: 20 Average loss: 71.1110
====> Test set loss: 71.0677
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{vae\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{p}{:}
    \PY{n}{BCE} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD}

\PY{k}{def} \PY{n+nf}{train\PYZus{}vae\PYZus{}with\PYZus{}realnvpflow}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}
    
    \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{p}{)}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
            \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{data}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{n}{vae\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
            \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
            \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{====\PYZgt{} Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ Average loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k+kn}{import} \PY{n}{save\PYZus{}image}\PY{p}{,} \PY{n}{make\PYZus{}grid}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k+kn}{import} \PY{n}{PCA}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{manifold} \PY{k+kn}{import} \PY{n}{TSNE}

\PY{k}{def} \PY{n+nf}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{use\PYZus{}pca}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        
        \PY{n}{z} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{samples} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{generate}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}

       
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
        \PY{n}{grid\PYZus{}img} \PY{o}{=} \PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{samples}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,} \PY{n}{nrow}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{grid\PYZus{}img}\PY{o}{.}\PY{n}{permute}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

      
        \PY{n}{latents} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{labels} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{for} \PY{n}{data}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{data\PYZus{}loader}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}
            \PY{n}{z}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
            \PY{n}{latents}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{z}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{labels}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{label}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}

        \PY{n}{latents} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
        \PY{n}{labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{labels}\PY{p}{)}

        
        \PY{k}{if} \PY{n}{model}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{\PYZgt{}} \PY{l+m+mi}{2}\PY{p}{:}
            \PY{k}{if} \PY{n}{use\PYZus{}pca}\PY{p}{:}
                \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{TSNE}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{latents} 

        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{latents\PYZus{}2d}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{latents\PYZus{}2d}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{labels}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Dimension 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Dimension 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Space Visualization}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{vae\PYZus{}with\PYZus{}realnvp} \PY{o}{=} \PY{n}{VAEWithRealNVP}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{train\PYZus{}vae\PYZus{}with\PYZus{}realnvpflow}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}realnvp}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
====> Epoch: 0 Average loss: 104.6459
====> Epoch: 1 Average loss: 78.2237
====> Epoch: 2 Average loss: 74.5231
====> Epoch: 3 Average loss: 72.8711
====> Epoch: 4 Average loss: 71.7555
====> Epoch: 5 Average loss: 70.7489
====> Epoch: 6 Average loss: 70.0991
====> Epoch: 7 Average loss: 69.4540
====> Epoch: 8 Average loss: 68.9480
====> Epoch: 9 Average loss: 68.5396
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}realnvp}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Analysis Based on the
plot:}\label{analysis-based-on-the-plot}

\paragraph{\texorpdfstring{1. \textbf{Sample
Quality}}{1. Sample Quality}}\label{sample-quality}

\begin{itemize}
\tightlist
\item
  \textbf{Observation}: Generated images are blurry with unclear
  details, making digits hard to recognize.
\item
  \textbf{Issues}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Undertraining}: Insufficient training may lead to poor
    sample quality.
  \item
    \textbf{VAE Limitations}: VAE may struggle with generating
    high-quality samples, especially with complex data.
  \item
    \textbf{RealNVP Adjustment}: Inadequate tuning of RealNVP may fail
    to capture the true data distribution.
  \end{itemize}
\item
  \textbf{Suggestions}:

  \begin{itemize}
  \tightlist
  \item
    Extend training duration.
  \item
    Fine-tune hyperparameters (e.g., learning rate, latent dimensions).
  \item
    Experiment with other flow models like Planar Flow or MAF.
  \end{itemize}
\end{itemize}

\paragraph{\texorpdfstring{2. \textbf{Latent Space
Structure}}{2. Latent Space Structure}}\label{latent-space-structure}

\begin{itemize}
\tightlist
\item
  \textbf{Observation}: Data points are evenly distributed but lack
  clear clustering; labels are mixed without distinct separation.
\item
  \textbf{Issues}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Misaligned Distribution}: RealNVP may not properly align
    latent space, causing poor category separation.
  \item
    \textbf{Overextended Space}: Excessive spread in latent space may
    degrade sample quality.
  \end{itemize}
\item
  \textbf{Suggestions}:

  \begin{itemize}
  \tightlist
  \item
    Adjust dimensionality reduction techniques (e.g., t-SNE, PCA).
  \item
    Increase regularization for a more compact latent space.
  \item
    Modify encoder and flow structure for better alignment with data
    distribution.
  \end{itemize}
\end{itemize}

\subsubsection{Summary}\label{summary}

\begin{itemize}
\tightlist
\item
  \textbf{Sample Quality}: Currently poor; needs more training or model
  adjustments.
\item
  \textbf{Latent Space}: Lacks clear separation; requires better
  structure alignment.
\end{itemize}

To improve results, consider fine-tuning hyperparameters, extending
training, or switching flow models for enhanced sample quality and
latent space structure.

    \subsubsection{Why RealNVP is Effective}\label{why-realnvp-is-effective}

\begin{itemize}
\tightlist
\item
  \textbf{Expressive Power}: Captures complex distributions through
  invertible affine transformations, fitting high-dimensional data
  better.
\item
  \textbf{Stable Training}: Simplifies Jacobian calculation, ensuring
  stable and efficient training.
\item
  \textbf{Latent Space}: Enhances latent space flexibility, leading to
  more realistic samples.
\item
  \textbf{Scalability}: Easily expands with more layers without
  increasing complexity.
\end{itemize}

\subsubsection{Summary}\label{summary}

RealNVP significantly boosts model performance, improving sample quality
and stability, especially in complex, high-dimensional tasks.

    \section{Combined Flow: Key Points}\label{combined-flow-key-points}

\textbf{Combined Flow} enhances a VAE by combining \textbf{RealNVP} and
\textbf{Planar Flows} to increase the flexibility and expressiveness of
the latent space.

\paragraph{Key Concepts:}\label{key-concepts}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hybrid Approach}:

  \begin{itemize}
  \tightlist
  \item
    Integrates \textbf{RealNVP} for global transformations and
    \textbf{Planar Flows} for local adjustments in the latent space,
    providing a balance of structure and flexibility.
  \end{itemize}
\item
  \textbf{Sequential Processing}:

  \begin{itemize}
  \tightlist
  \item
    Applies flows in a sequence, first transforming with RealNVP for
    robust shaping, then fine-tuning with Planar Flows.
  \end{itemize}
\item
  \textbf{Improved Expressiveness}:

  \begin{itemize}
  \tightlist
  \item
    This combination allows the model to better capture complex data
    distributions, enhancing the VAE's ability to generate diverse and
    accurate samples.
  \end{itemize}
\end{enumerate}

The Combined Flow approach makes the VAE more adaptable to complex data
patterns, leading to superior generative performance.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{CombinedFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{num\PYZus{}realnvp\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{num\PYZus{}planar\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{CombinedFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{realnvp\PYZus{}flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{RealNVPFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}realnvp\PYZus{}flows}\PY{p}{)}\PY{p}{]}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{planar\PYZus{}flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{PlanarFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}planar\PYZus{}flows}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{l+m+mi}{0}
        
       
        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{realnvp\PYZus{}flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{det\PYZus{}jacobian}
        
       
        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{planar\PYZus{}flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{det\PYZus{}jacobian} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}
        
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    
    
\PY{k}{class} \PY{n+nc}{VAEWithCombinedFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}realnvp\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{num\PYZus{}planar\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEWithCombinedFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{latent\PYZus{}dim}

       
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}  

      
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}

      
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow} \PY{o}{=} \PY{n}{CombinedFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}realnvp\PYZus{}flows}\PY{p}{,} \PY{n}{num\PYZus{}planar\PYZus{}flows}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{h1} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc21}\PY{p}{(}\PY{n}{h1}\PY{p}{)}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc22}\PY{p}{(}\PY{n}{h1}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{n}{z0} \PY{o}{=} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}
        
        
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{z0}\PY{p}{)}
        
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{h3} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc4}\PY{p}{(}\PY{n}{h3}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}
    
    \PY{k}{def} \PY{n+nf}{generate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}

 
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{VAEWithCombinedFlow}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}


\PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}


\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{21}\PY{p}{)}\PY{p}{:}
    \PY{n}{train}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
    \PY{n}{test}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train Epoch: 1 [0/60000]        Loss: 554.683228
Train Epoch: 1 [6400/60000]     Loss: 184.731567
Train Epoch: 1 [12800/60000]    Loss: 171.809280
Train Epoch: 1 [19200/60000]    Loss: 150.241943
Train Epoch: 1 [25600/60000]    Loss: 137.676071
Train Epoch: 1 [32000/60000]    Loss: 131.328018
Train Epoch: 1 [38400/60000]    Loss: 110.952324
Train Epoch: 1 [44800/60000]    Loss: 123.687164
Train Epoch: 1 [51200/60000]    Loss: 121.474014
Train Epoch: 1 [57600/60000]    Loss: 113.123947
====> Epoch: 1 Average loss: 155.9750
====> Test set loss: 114.4605
Train Epoch: 2 [0/60000]        Loss: 112.696976
Train Epoch: 2 [6400/60000]     Loss: 110.119186
Train Epoch: 2 [12800/60000]    Loss: 110.233643
Train Epoch: 2 [19200/60000]    Loss: 107.310501
Train Epoch: 2 [25600/60000]    Loss: 107.445679
Train Epoch: 2 [32000/60000]    Loss: 109.807159
Train Epoch: 2 [38400/60000]    Loss: 104.083588
Train Epoch: 2 [44800/60000]    Loss: 106.503647
Train Epoch: 2 [51200/60000]    Loss: 102.831657
Train Epoch: 2 [57600/60000]    Loss: 104.082207
====> Epoch: 2 Average loss: 106.9139
====> Test set loss: 100.1970
Train Epoch: 3 [0/60000]        Loss: 106.538780
Train Epoch: 3 [6400/60000]     Loss: 105.090790
Train Epoch: 3 [12800/60000]    Loss: 95.988937
Train Epoch: 3 [19200/60000]    Loss: 96.323090
Train Epoch: 3 [25600/60000]    Loss: 96.161621
Train Epoch: 3 [32000/60000]    Loss: 103.012619
Train Epoch: 3 [38400/60000]    Loss: 88.866241
Train Epoch: 3 [44800/60000]    Loss: 99.646896
Train Epoch: 3 [51200/60000]    Loss: 89.380394
Train Epoch: 3 [57600/60000]    Loss: 94.285294
====> Epoch: 3 Average loss: 97.4473
====> Test set loss: 92.9738
Train Epoch: 4 [0/60000]        Loss: 97.023659
Train Epoch: 4 [6400/60000]     Loss: 98.314636
Train Epoch: 4 [12800/60000]    Loss: 95.619331
Train Epoch: 4 [19200/60000]    Loss: 96.575546
Train Epoch: 4 [25600/60000]    Loss: 88.507828
Train Epoch: 4 [32000/60000]    Loss: 83.951447
Train Epoch: 4 [38400/60000]    Loss: 88.534752
Train Epoch: 4 [44800/60000]    Loss: 96.937416
Train Epoch: 4 [51200/60000]    Loss: 98.214996
Train Epoch: 4 [57600/60000]    Loss: 96.955933
====> Epoch: 4 Average loss: 92.1485
====> Test set loss: 89.8049
Train Epoch: 5 [0/60000]        Loss: 99.415001
Train Epoch: 5 [6400/60000]     Loss: 98.247856
Train Epoch: 5 [12800/60000]    Loss: 82.017876
Train Epoch: 5 [19200/60000]    Loss: 81.730202
Train Epoch: 5 [25600/60000]    Loss: 85.305161
Train Epoch: 5 [32000/60000]    Loss: 87.244423
Train Epoch: 5 [38400/60000]    Loss: 99.490639
Train Epoch: 5 [44800/60000]    Loss: 97.741051
Train Epoch: 5 [51200/60000]    Loss: 91.218254
Train Epoch: 5 [57600/60000]    Loss: 93.872620
====> Epoch: 5 Average loss: 90.2391
====> Test set loss: 87.4474
Train Epoch: 6 [0/60000]        Loss: 87.471054
Train Epoch: 6 [6400/60000]     Loss: 86.367889
Train Epoch: 6 [12800/60000]    Loss: 90.747528
Train Epoch: 6 [19200/60000]    Loss: 81.581001
Train Epoch: 6 [25600/60000]    Loss: 91.844986
Train Epoch: 6 [32000/60000]    Loss: 91.289772
Train Epoch: 6 [38400/60000]    Loss: 84.526520
Train Epoch: 6 [44800/60000]    Loss: 75.065208
Train Epoch: 6 [51200/60000]    Loss: 81.812294
Train Epoch: 6 [57600/60000]    Loss: 82.383621
====> Epoch: 6 Average loss: 85.0950
====> Test set loss: 83.2832
Train Epoch: 7 [0/60000]        Loss: 78.759384
Train Epoch: 7 [6400/60000]     Loss: 88.426918
Train Epoch: 7 [12800/60000]    Loss: 87.214722
Train Epoch: 7 [19200/60000]    Loss: 94.188202
Train Epoch: 7 [25600/60000]    Loss: 91.327354
Train Epoch: 7 [32000/60000]    Loss: 93.709488
Train Epoch: 7 [38400/60000]    Loss: 86.414078
Train Epoch: 7 [44800/60000]    Loss: 90.132980
Train Epoch: 7 [51200/60000]    Loss: 82.892159
Train Epoch: 7 [57600/60000]    Loss: 78.056946
====> Epoch: 7 Average loss: 86.5070
====> Test set loss: 83.3568
Train Epoch: 8 [0/60000]        Loss: 81.268845
Train Epoch: 8 [6400/60000]     Loss: 86.355415
Train Epoch: 8 [12800/60000]    Loss: 88.578873
Train Epoch: 8 [19200/60000]    Loss: 76.330254
Train Epoch: 8 [25600/60000]    Loss: 77.311584
Train Epoch: 8 [32000/60000]    Loss: 83.957970
Train Epoch: 8 [38400/60000]    Loss: 85.357002
Train Epoch: 8 [44800/60000]    Loss: 84.136040
Train Epoch: 8 [51200/60000]    Loss: 80.685333
Train Epoch: 8 [57600/60000]    Loss: 81.577484
====> Epoch: 8 Average loss: 81.8842
====> Test set loss: 79.3167
Train Epoch: 9 [0/60000]        Loss: 76.688774
Train Epoch: 9 [6400/60000]     Loss: 82.693024
Train Epoch: 9 [12800/60000]    Loss: 77.646164
Train Epoch: 9 [19200/60000]    Loss: 87.219864
Train Epoch: 9 [25600/60000]    Loss: 72.522102
Train Epoch: 9 [32000/60000]    Loss: 73.026901
Train Epoch: 9 [38400/60000]    Loss: 85.154419
Train Epoch: 9 [44800/60000]    Loss: 79.866486
Train Epoch: 9 [51200/60000]    Loss: 75.896805
Train Epoch: 9 [57600/60000]    Loss: 83.892303
====> Epoch: 9 Average loss: 80.0564
====> Test set loss: 78.1605
Train Epoch: 10 [0/60000]       Loss: 80.098328
Train Epoch: 10 [6400/60000]    Loss: 75.751251
Train Epoch: 10 [12800/60000]   Loss: 72.295181
Train Epoch: 10 [19200/60000]   Loss: 77.422722
Train Epoch: 10 [25600/60000]   Loss: 74.601151
Train Epoch: 10 [32000/60000]   Loss: 69.187622
Train Epoch: 10 [38400/60000]   Loss: 79.912384
Train Epoch: 10 [44800/60000]   Loss: 80.334618
Train Epoch: 10 [51200/60000]   Loss: 76.856216
Train Epoch: 10 [57600/60000]   Loss: 77.151398
====> Epoch: 10 Average loss: 78.5615
====> Test set loss: 77.6566
Train Epoch: 11 [0/60000]       Loss: 81.982285
Train Epoch: 11 [6400/60000]    Loss: 73.234245
Train Epoch: 11 [12800/60000]   Loss: 82.668327
Train Epoch: 11 [19200/60000]   Loss: 75.824127
Train Epoch: 11 [25600/60000]   Loss: 69.550858
Train Epoch: 11 [32000/60000]   Loss: 81.478050
Train Epoch: 11 [38400/60000]   Loss: 84.464844
Train Epoch: 11 [44800/60000]   Loss: 75.130302
Train Epoch: 11 [51200/60000]   Loss: 78.777954
Train Epoch: 11 [57600/60000]   Loss: 74.127014
====> Epoch: 11 Average loss: 77.6663
====> Test set loss: 76.0430
Train Epoch: 12 [0/60000]       Loss: 76.567261
Train Epoch: 12 [6400/60000]    Loss: 74.724785
Train Epoch: 12 [12800/60000]   Loss: 80.968399
Train Epoch: 12 [19200/60000]   Loss: 73.646362
Train Epoch: 12 [25600/60000]   Loss: 72.110519
Train Epoch: 12 [32000/60000]   Loss: 84.050995
Train Epoch: 12 [38400/60000]   Loss: 80.911552
Train Epoch: 12 [44800/60000]   Loss: 75.999695
Train Epoch: 12 [51200/60000]   Loss: 78.087769
Train Epoch: 12 [57600/60000]   Loss: 79.404480
====> Epoch: 12 Average loss: 76.3535
====> Test set loss: 74.5274
Train Epoch: 13 [0/60000]       Loss: 72.040390
Train Epoch: 13 [6400/60000]    Loss: 68.971397
Train Epoch: 13 [12800/60000]   Loss: 77.156784
Train Epoch: 13 [19200/60000]   Loss: 67.430946
Train Epoch: 13 [25600/60000]   Loss: 74.697800
Train Epoch: 13 [32000/60000]   Loss: 78.527344
Train Epoch: 13 [38400/60000]   Loss: 76.572273
Train Epoch: 13 [44800/60000]   Loss: 74.659531
Train Epoch: 13 [51200/60000]   Loss: 78.167976
Train Epoch: 13 [57600/60000]   Loss: 68.736206
====> Epoch: 13 Average loss: 76.1422
====> Test set loss: 74.2728
Train Epoch: 14 [0/60000]       Loss: 75.671280
Train Epoch: 14 [6400/60000]    Loss: 75.000229
Train Epoch: 14 [12800/60000]   Loss: 71.657639
Train Epoch: 14 [19200/60000]   Loss: 81.340576
Train Epoch: 14 [25600/60000]   Loss: 77.844017
Train Epoch: 14 [32000/60000]   Loss: 72.911522
Train Epoch: 14 [38400/60000]   Loss: 77.727493
Train Epoch: 14 [44800/60000]   Loss: 72.610031
Train Epoch: 14 [51200/60000]   Loss: 73.870567
Train Epoch: 14 [57600/60000]   Loss: 82.159706
====> Epoch: 14 Average loss: 75.3801
====> Test set loss: 72.7241
Train Epoch: 15 [0/60000]       Loss: 70.901443
Train Epoch: 15 [6400/60000]    Loss: 78.227707
Train Epoch: 15 [12800/60000]   Loss: 74.152496
Train Epoch: 15 [19200/60000]   Loss: 75.872437
Train Epoch: 15 [25600/60000]   Loss: 76.167267
Train Epoch: 15 [32000/60000]   Loss: 75.325653
Train Epoch: 15 [38400/60000]   Loss: 69.597435
Train Epoch: 15 [44800/60000]   Loss: 73.933090
Train Epoch: 15 [51200/60000]   Loss: 70.040131
Train Epoch: 15 [57600/60000]   Loss: 76.997375
====> Epoch: 15 Average loss: 73.9142
====> Test set loss: 71.5064
Train Epoch: 16 [0/60000]       Loss: 73.070847
Train Epoch: 16 [6400/60000]    Loss: 70.700195
Train Epoch: 16 [12800/60000]   Loss: 76.554169
Train Epoch: 16 [19200/60000]   Loss: 75.926964
Train Epoch: 16 [25600/60000]   Loss: 70.198898
Train Epoch: 16 [32000/60000]   Loss: 72.402946
Train Epoch: 16 [38400/60000]   Loss: 72.849472
Train Epoch: 16 [44800/60000]   Loss: 74.073830
Train Epoch: 16 [51200/60000]   Loss: 87.790138
Train Epoch: 16 [57600/60000]   Loss: 86.250793
====> Epoch: 16 Average loss: 74.8463
====> Test set loss: 79.2954
Train Epoch: 17 [0/60000]       Loss: 80.237434
Train Epoch: 17 [6400/60000]    Loss: 77.384048
Train Epoch: 17 [12800/60000]   Loss: 73.700539
Train Epoch: 17 [19200/60000]   Loss: 71.976204
Train Epoch: 17 [25600/60000]   Loss: 71.395805
Train Epoch: 17 [32000/60000]   Loss: 72.496597
Train Epoch: 17 [38400/60000]   Loss: 72.667999
Train Epoch: 17 [44800/60000]   Loss: 63.992615
Train Epoch: 17 [51200/60000]   Loss: 75.043274
Train Epoch: 17 [57600/60000]   Loss: 68.377281
====> Epoch: 17 Average loss: 73.6142
====> Test set loss: 70.5415
Train Epoch: 18 [0/60000]       Loss: 72.964272
Train Epoch: 18 [6400/60000]    Loss: 68.088898
Train Epoch: 18 [12800/60000]   Loss: 62.519028
Train Epoch: 18 [19200/60000]   Loss: 69.428642
Train Epoch: 18 [25600/60000]   Loss: 69.540756
Train Epoch: 18 [32000/60000]   Loss: 66.446060
Train Epoch: 18 [38400/60000]   Loss: 63.148476
Train Epoch: 18 [44800/60000]   Loss: 78.982994
Train Epoch: 18 [51200/60000]   Loss: 78.143791
Train Epoch: 18 [57600/60000]   Loss: 74.094345
====> Epoch: 18 Average loss: 72.3236
====> Test set loss: 70.1435
Train Epoch: 19 [0/60000]       Loss: 74.255028
Train Epoch: 19 [6400/60000]    Loss: 77.392998
Train Epoch: 19 [12800/60000]   Loss: 67.859283
Train Epoch: 19 [19200/60000]   Loss: 66.390167
Train Epoch: 19 [25600/60000]   Loss: 69.555641
Train Epoch: 19 [32000/60000]   Loss: 65.826126
Train Epoch: 19 [38400/60000]   Loss: 72.179581
Train Epoch: 19 [44800/60000]   Loss: 73.133675
Train Epoch: 19 [51200/60000]   Loss: 71.185532
Train Epoch: 19 [57600/60000]   Loss: 70.920891
====> Epoch: 19 Average loss: 70.2878
====> Test set loss: 68.2532
Train Epoch: 20 [0/60000]       Loss: 67.096878
Train Epoch: 20 [6400/60000]    Loss: 73.898651
Train Epoch: 20 [12800/60000]   Loss: 60.570786
Train Epoch: 20 [19200/60000]   Loss: 71.570930
Train Epoch: 20 [25600/60000]   Loss: 73.037628
Train Epoch: 20 [32000/60000]   Loss: 65.029144
Train Epoch: 20 [38400/60000]   Loss: 66.659821
Train Epoch: 20 [44800/60000]   Loss: 62.536995
Train Epoch: 20 [51200/60000]   Loss: 69.492996
Train Epoch: 20 [57600/60000]   Loss: 65.896751
====> Epoch: 20 Average loss: 69.8619
====> Test set loss: 68.4026
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train\PYZus{}vae\PYZus{}with\PYZus{}combined\PYZus{}flow}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

    \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{p}{)}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
            \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{data}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{n}{vae\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
            \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
            \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{====\PYZgt{} Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ Average loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{input\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{784}
\PY{n}{hidden\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{400}
\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{50}
\PY{n}{num\PYZus{}realnvp\PYZus{}flows} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{num\PYZus{}planar\PYZus{}flows} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{10}


\PY{n}{vae\PYZus{}with\PYZus{}combined\PYZus{}flow} \PY{o}{=} \PY{n}{VAEWithCombinedFlow}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}realnvp\PYZus{}flows}\PY{o}{=}\PY{n}{num\PYZus{}realnvp\PYZus{}flows}\PY{p}{,} \PY{n}{num\PYZus{}planar\PYZus{}flows}\PY{o}{=}\PY{n}{num\PYZus{}planar\PYZus{}flows}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{train\PYZus{}vae\PYZus{}with\PYZus{}combined\PYZus{}flow}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}combined\PYZus{}flow}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{o}{=}\PY{n}{num\PYZus{}epochs}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
====> Epoch: 0 Average loss: 98.7750
====> Epoch: 1 Average loss: 66.8329
====> Epoch: 2 Average loss: 62.8472
====> Epoch: 3 Average loss: 59.6244
====> Epoch: 4 Average loss: 58.3282
====> Epoch: 5 Average loss: 61.2480
====> Epoch: 6 Average loss: 59.2590
====> Epoch: 7 Average loss: 56.5406
====> Epoch: 8 Average loss: 55.9999
====> Epoch: 9 Average loss: 56.4312
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}combined\PYZus{}flow}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Key Points}\label{key-points}

\paragraph{\texorpdfstring{1. \textbf{Sample
Quality}}{1. Sample Quality}}\label{sample-quality}

\begin{itemize}
\tightlist
\item
  \textbf{Blurry Images}: Generated images are unclear with poor detail.
\item
  \textbf{Undertraining}: Model likely needs more training.
\item
  \textbf{Complexity Issues}: CombinedFlow may be too complex, affecting
  stability.
\end{itemize}

\paragraph{\texorpdfstring{2. \textbf{Latent Space
Structure}}{2. Latent Space Structure}}\label{latent-space-structure}

\begin{itemize}
\tightlist
\item
  \textbf{Poor Category Separation}: Latent space lacks distinct
  clusters.
\item
  \textbf{Balancing Issues}: KL divergence and flow contributions may
  not be well-balanced.
\end{itemize}

\subsubsection{Suggestions}\label{suggestions}

\begin{itemize}
\tightlist
\item
  \textbf{Increase Training}: Extend epochs for better learning.
\item
  \textbf{Adjust Flow Model}: Simplify or reconfigure flows for improved
  stability and separation.
\item
  \textbf{Enhance Regularization}: Strengthen KL divergence to improve
  latent space clarity.
\end{itemize}

    \subsubsection{Introducing Adversarial Training with
VAE-GAN}\label{introducing-adversarial-training-with-vae-gan}

VAE-GAN is a powerful approach that combines the strengths of a
Variational Autoencoder (VAE) with a Generative Adversarial Network
(GAN). This hybrid model leverages the generative capabilities of the
VAE and the adversarial training of the GAN to enhance the quality,
realism, and diversity of generated samples. By integrating these two
models, VAE-GAN aims to overcome the limitations of each individual
model and produce more accurate and high-fidelity outputs.

\paragraph{VAE-GAN Architecture}\label{vae-gan-architecture}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Variational Autoencoder (VAE)}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Encoder}: Maps input data to a latent space by learning the
    mean and variance of a Gaussian distribution. This probabilistic
    mapping enables the model to capture complex data distributions and
    generate diverse samples.
  \item
    \textbf{Latent Space}: The VAE samples from the latent space to
    generate new data points. This space represents the underlying
    features learned from the input data.
  \item
    \textbf{Decoder}: Reconstructs the input data from the sampled
    latent variables, generating data that resembles the original input.
  \end{itemize}
\item
  \textbf{Generative Adversarial Network (GAN)}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Generator (VAE)}: In the VAE-GAN setup, the VAE's decoder
    serves as the generator. It generates new samples from the latent
    space.
  \item
    \textbf{Discriminator}: The GAN's discriminator evaluates the
    authenticity of the samples by distinguishing between real data and
    the data generated by the VAE. It provides feedback to the
    generator, pushing it to produce more realistic samples.
  \item
    \textbf{Adversarial Training}: The VAE and discriminator are trained
    in a competitive manner. The discriminator tries to correctly
    identify real and fake samples, while the VAE adjusts its parameters
    to generate samples that can fool the discriminator.
  \end{itemize}
\end{enumerate}

\paragraph{Implementation Strategy}\label{implementation-strategy}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Use VAE as the Generator}:

  \begin{itemize}
  \tightlist
  \item
    The VAE generates samples by sampling from the learned latent space.
    The decoder reconstructs these samples, aiming to produce data that
    closely resembles the training data.
  \end{itemize}
\item
  \textbf{GAN Discriminator for Distribution Evaluation}:

  \begin{itemize}
  \tightlist
  \item
    The GAN's discriminator is trained to evaluate the differences
    between real and generated samples. It assesses how closely the
    generated samples match the true data distribution, providing a
    measure of sample quality.
  \end{itemize}
\item
  \textbf{Adversarial Training for Realism}:

  \begin{itemize}
  \tightlist
  \item
    Adversarial training is employed to make the VAE-generated samples
    more realistic. The discriminator's feedback is used to refine the
    VAE, improving the overall quality of the generated data.
  \end{itemize}
\end{enumerate}

\paragraph{Benefits of VAE-GAN}\label{benefits-of-vae-gan}

\begin{itemize}
\tightlist
\item
  \textbf{Improved Sample Quality}: By combining VAE's generative
  capabilities with GAN's adversarial training, VAE-GAN produces higher
  quality samples with sharper details and fewer artifacts.
\item
  \textbf{Diverse Sample Generation}: The VAE's latent space sampling
  ensures diversity in the generated data, while the GAN's discriminator
  ensures that this diversity does not come at the cost of realism.
\item
  \textbf{Overcoming Limitations}: VAE-GAN mitigates common issues in
  VAEs, such as blurry outputs, by incorporating the GAN's ability to
  generate sharp and realistic images. It also addresses the mode
  collapse issue in GANs by utilizing VAE's structured latent space.
\end{itemize}

    Implementing the Discriminator First, we implement a discriminator to
distinguish between real samples and generated samples. We can use
Spectral Normalization to enhance the stability of the discriminator.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}

\PY{k}{class} \PY{n+nc}{SpectralNorm}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{module}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{power\PYZus{}iterations}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{SpectralNorm}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module} \PY{o}{=} \PY{n}{module}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{=} \PY{n}{name}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{power\PYZus{}iterations} \PY{o}{=} \PY{n}{power\PYZus{}iterations}
        \PY{k}{if} \PY{o+ow}{not} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}made\PYZus{}params}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}make\PYZus{}params}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{\PYZus{}update\PYZus{}u\PYZus{}v}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n}{u} \PY{o}{=} \PY{n+nb}{getattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}u}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{v} \PY{o}{=} \PY{n+nb}{getattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}v}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{w} \PY{o}{=} \PY{n+nb}{getattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}bar}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

        \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{power\PYZus{}iterations}\PY{p}{)}\PY{p}{:}
            \PY{n}{v} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{normalize}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{t}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{u}\PY{p}{)}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}12}\PY{p}{)}
            \PY{n}{u} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{normalize}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{v}\PY{p}{)}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}12}\PY{p}{)}

        \PY{n}{sigma} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{u}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{v}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{setattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name}\PY{p}{,} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{w} \PY{o}{/} \PY{n}{sigma}\PY{o}{.}\PY{n}{expand\PYZus{}as}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{\PYZus{}made\PYZus{}params}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{k}{try}\PY{p}{:}
            \PY{n}{u} \PY{o}{=} \PY{n+nb}{getattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}u}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{v} \PY{o}{=} \PY{n+nb}{getattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}v}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{w} \PY{o}{=} \PY{n+nb}{getattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}bar}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{k+kc}{True}
        \PY{k}{except} \PY{n+ne}{AttributeError}\PY{p}{:}
            \PY{k}{return} \PY{k+kc}{False}

    \PY{k}{def} \PY{n+nf}{\PYZus{}make\PYZus{}params}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n}{w} \PY{o}{=} \PY{n+nb}{getattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name}\PY{p}{)}

        \PY{n}{height} \PY{o}{=} \PY{n}{w}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{width} \PY{o}{=} \PY{n}{w}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{height}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

        \PY{n}{u} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{normalize}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{new\PYZus{}empty}\PY{p}{(}\PY{n}{height}\PY{p}{)}\PY{o}{.}\PY{n}{normal\PYZus{}}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}12}\PY{p}{)}
        \PY{n}{v} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{normalize}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{new\PYZus{}empty}\PY{p}{(}\PY{n}{width}\PY{p}{)}\PY{o}{.}\PY{n}{normal\PYZus{}}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}12}\PY{p}{)}
        \PY{n}{w\PYZus{}bar} \PY{o}{=} \PY{n}{w}\PY{o}{.}\PY{n}{data}

        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{o}{.}\PY{n}{register\PYZus{}buffer}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}u}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{u}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{o}{.}\PY{n}{register\PYZus{}buffer}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}v}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{v}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{o}{.}\PY{n}{register\PYZus{}parameter}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{name} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}bar}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{w\PYZus{}bar}\PY{p}{)}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}update\PYZus{}u\PYZus{}v}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{o}{*}\PY{n}{args}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}update\PYZus{}u\PYZus{}v}\PY{p}{(}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{module}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{)}


\PY{k}{class} \PY{n+nc}{DiscriminatorWithSpectralNorm}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{DiscriminatorWithSpectralNorm}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{SpectralNorm}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2} \PY{o}{=} \PY{n}{SpectralNorm}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{SpectralNorm}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 确保输入张量展平为 (batch\PYZus{}size, 784)}
        \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{leaky\PYZus{}relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{)}
        \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{leaky\PYZus{}relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{)}
        \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Modified generator (VAEWithCombinedFlow) VAEWithCombinedFlow is used as
a generator in which the flow has been augmented. We use it as a
generator part for adversarial training of GAN.

    Implementing a VAE-GAN Hybrid Model To combine VAE with GAN, we need a
combinatorial model that handles both the reconstruction of VAE and the
adversarial loss of GAN.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{VAEGAN}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{vae}\PY{p}{,} \PY{n}{discriminator}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEGAN}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{vae} \PY{o}{=} \PY{n}{vae}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{discriminator} \PY{o}{=} \PY{n}{discriminator}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{vae}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{return} \PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}
    
    \PY{k}{def} \PY{n+nf}{generate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{vae}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Define the loss function We need a combined loss function that takes
into account the reconstruction error of the VAE, the KL dispersion, and
the adversarial loss of the GAN.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{vae\PYZus{}gan\PYZus{}loss}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{,} \PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{fake\PYZus{}output}\PY{p}{)}\PY{p}{:}
    \PY{n}{BCE} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{n}{KLD} \PY{o}{\PYZhy{}} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} 对抗性损失}
    \PY{n}{adv\PYZus{}loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{fake\PYZus{}output}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{fake\PYZus{}output}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD} \PY{o}{+} \PY{n}{adv\PYZus{}loss}

\PY{k}{def} \PY{n+nf}{discriminator\PYZus{}loss}\PY{p}{(}\PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{fake\PYZus{}output}\PY{p}{)}\PY{p}{:}
    \PY{n}{real\PYZus{}loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{real\PYZus{}output}\PY{p}{)}\PY{p}{)}
    \PY{n}{fake\PYZus{}loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{fake\PYZus{}output}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{fake\PYZus{}output}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{real\PYZus{}loss} \PY{o}{+} \PY{n}{fake\PYZus{}loss}
\end{Verbatim}
\end{tcolorbox}

    Training the VAE-GAN model Next, a training loop is defined in which
both the VAE and the discriminator are optimized.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{50}
\PY{n}{vae} \PY{o}{=} \PY{n}{VAEWithCombinedFlow}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}realnvp\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{num\PYZus{}planar\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{discriminator} \PY{o}{=} \PY{n}{DiscriminatorWithSpectralNorm}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{vae\PYZus{}gan} \PY{o}{=} \PY{n}{VAEGAN}\PY{p}{(}\PY{n}{vae}\PY{p}{,} \PY{n}{discriminator}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{n}{optimizer\PYZus{}g} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{vae}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,} \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}\PY{p}{)}
\PY{n}{optimizer\PYZus{}d} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{discriminator}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,} \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}\PY{p}{)}

\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{64}

\PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{20}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{data\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{dataset}\PY{o}{=}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}


\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{real\PYZus{}images}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{p}{)}\PY{p}{:}
        \PY{n}{real\PYZus{}images} \PY{o}{=} \PY{n}{real\PYZus{}images}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{real\PYZus{}images}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
        
        
        \PY{n}{recon\PYZus{}images}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{p}{(}\PY{n}{real\PYZus{}images}\PY{p}{)}

      
        \PY{n}{z} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{fake\PYZus{}images} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{generate}\PY{p}{(}\PY{n}{z}\PY{p}{)}
      
       
        \PY{n}{real\PYZus{}output} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{discriminator}\PY{p}{(}\PY{n}{real\PYZus{}images}\PY{p}{)}
        \PY{n}{fake\PYZus{}output} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{discriminator}\PY{p}{(}\PY{n}{fake\PYZus{}images}\PY{p}{)}
        
      
        \PY{n}{d\PYZus{}loss} \PY{o}{=} \PY{n}{discriminator\PYZus{}loss}\PY{p}{(}\PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{fake\PYZus{}output}\PY{p}{)}
        
      
        \PY{n}{optimizer\PYZus{}d}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{d\PYZus{}loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer\PYZus{}d}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
        
      
        \PY{n}{fake\PYZus{}output} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{discriminator}\PY{p}{(}\PY{n}{recon\PYZus{}images}\PY{p}{)}
        \PY{n}{g\PYZus{}loss} \PY{o}{=} \PY{n}{vae\PYZus{}gan\PYZus{}loss}\PY{p}{(}\PY{n}{recon\PYZus{}images}\PY{p}{,} \PY{n}{real\PYZus{}images}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{,} \PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{fake\PYZus{}output}\PY{p}{)}
        
        
        \PY{n}{optimizer\PYZus{}g}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{g\PYZus{}loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer\PYZus{}g}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch [}\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}epochs}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{], d\PYZus{}loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{d\PYZus{}loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, g\PYZus{}loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{g\PYZus{}loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch [0/20], d\_loss: 1.1445033550262451, g\_loss: 3973.83056640625
Epoch [1/20], d\_loss: 1.0391623973846436, g\_loss: 3057.28759765625
Epoch [2/20], d\_loss: 1.008840799331665, g\_loss: 2957.677978515625
Epoch [3/20], d\_loss: 0.9954007863998413, g\_loss: 2772.764892578125
Epoch [4/20], d\_loss: 0.9905545115470886, g\_loss: 2512.4111328125
Epoch [5/20], d\_loss: 1.001739263534546, g\_loss: 2468.214111328125
Epoch [6/20], d\_loss: 1.0247597694396973, g\_loss: 2452.9541015625
Epoch [7/20], d\_loss: 1.0499069690704346, g\_loss: 2650.5126953125
Epoch [8/20], d\_loss: 1.0331026315689087, g\_loss: 2039.690185546875
Epoch [9/20], d\_loss: 1.053390622138977, g\_loss: 2079.548828125
Epoch [10/20], d\_loss: 1.0526208877563477, g\_loss: 2269.3388671875
Epoch [11/20], d\_loss: 1.0442800521850586, g\_loss: 2239.904052734375
Epoch [12/20], d\_loss: 1.0597752332687378, g\_loss: 2249.018310546875
Epoch [13/20], d\_loss: 1.0615637302398682, g\_loss: 1974.9710693359375
Epoch [14/20], d\_loss: 1.0663193464279175, g\_loss: 1846.298828125
Epoch [15/20], d\_loss: 1.0616655349731445, g\_loss: 1943.9842529296875
Epoch [16/20], d\_loss: 1.1132010221481323, g\_loss: 1994.5111083984375
Epoch [17/20], d\_loss: 1.0743820667266846, g\_loss: 1673.5137939453125
Epoch [18/20], d\_loss: 1.0929055213928223, g\_loss: 1944.426513671875
Epoch [19/20], d\_loss: 1.0973994731903076, g\_loss: 2003.361083984375
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{vae\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{p}{:}
    \PY{n}{BCE} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{n}{KLD} \PY{o}{\PYZhy{}} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train\PYZus{}vae}\PY{p}{(}\PY{n}{vae}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
    \PY{n}{vae}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

    \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{p}{)}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
            \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae}\PY{p}{(}\PY{n}{data}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{n}{vae\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
            \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
            \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{====\PYZgt{} Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ Average loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{test\PYZus{}vae}\PY{p}{(}\PY{n}{vae}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{test\PYZus{}vae}\PY{p}{(}\PY{n}{vae}\PY{p}{)}\PY{p}{:}
    \PY{n}{vae}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
    \PY{n}{test\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{data\PYZus{}loader}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae}\PY{p}{(}\PY{n}{data}\PY{p}{)}
            \PY{n}{test\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{vae\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
    \PY{n}{test\PYZus{}loss} \PY{o}{/}\PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{====\PYZgt{} Test set loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{test\PYZus{}loss}\PY{l+s+si}{:}\PY{l+s+s2}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{test\PYZus{}loss}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train\PYZus{}vae\PYZus{}gan}\PY{p}{(}\PY{n}{vae\PYZus{}gan}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
    \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{n}{optimizer\PYZus{}g} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{vae}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,} \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}\PY{p}{)}
    \PY{n}{optimizer\PYZus{}d} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{discriminator}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,} \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}\PY{p}{)}

    \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{train\PYZus{}loss\PYZus{}g} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{train\PYZus{}loss\PYZus{}d} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{real\PYZus{}images}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{p}{)}\PY{p}{:}
            \PY{n}{real\PYZus{}images} \PY{o}{=} \PY{n}{real\PYZus{}images}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{real\PYZus{}images}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

          
            \PY{n}{recon\PYZus{}images}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{p}{(}\PY{n}{real\PYZus{}images}\PY{p}{)}

           
            \PY{n}{z} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{fake\PYZus{}images} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{generate}\PY{p}{(}\PY{n}{z}\PY{p}{)}

           
            \PY{n}{real\PYZus{}output} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{discriminator}\PY{p}{(}\PY{n}{real\PYZus{}images}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}  
            \PY{n}{fake\PYZus{}output} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{discriminator}\PY{p}{(}\PY{n}{fake\PYZus{}images}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}  

            
            \PY{n}{d\PYZus{}loss} \PY{o}{=} \PY{n}{discriminator\PYZus{}loss}\PY{p}{(}\PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{fake\PYZus{}output}\PY{p}{)}
            \PY{n}{train\PYZus{}loss\PYZus{}d} \PY{o}{+}\PY{o}{=} \PY{n}{d\PYZus{}loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}

            
            \PY{n}{optimizer\PYZus{}d}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
            \PY{n}{d\PYZus{}loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
            \PY{n}{optimizer\PYZus{}d}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

           
            \PY{n}{fake\PYZus{}output} \PY{o}{=} \PY{n}{vae\PYZus{}gan}\PY{o}{.}\PY{n}{discriminator}\PY{p}{(}\PY{n}{recon\PYZus{}images}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}  
            \PY{n}{g\PYZus{}loss} \PY{o}{=} \PY{n}{vae\PYZus{}gan\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}images}\PY{p}{,} \PY{n}{real\PYZus{}images}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{,} \PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{fake\PYZus{}output}\PY{p}{)}
            \PY{n}{train\PYZus{}loss\PYZus{}g} \PY{o}{+}\PY{o}{=} \PY{n}{g\PYZus{}loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}

           
            \PY{n}{optimizer\PYZus{}g}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
            \PY{n}{g\PYZus{}loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
            \PY{n}{optimizer\PYZus{}g}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch [}\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}epochs}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{], d\PYZus{}loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss\PYZus{}d}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, g\PYZus{}loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss\PYZus{}g}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}loader}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{.4f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k}{def} \PY{n+nf}{generate\PYZus{}samples}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{n}{z} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{samples} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{generate}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
        \PY{n}{save\PYZus{}image}\PY{p}{(}\PY{n}{samples}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{generated\PYZus{}samples.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nrow}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}

   
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{samples}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Initializing and Training a VAE Model}

\PY{n}{vae} \PY{o}{=} \PY{n}{VAEWithCombinedFlow}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{train\PYZus{}vae}\PY{p}{(}\PY{n}{vae}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
====> Epoch: 0 Average loss: 91.7329
====> Test set loss: 68.6578
====> Epoch: 1 Average loss: 64.8647
====> Test set loss: 61.8764
====> Epoch: 2 Average loss: 61.2163
====> Test set loss: 59.7085
====> Epoch: 3 Average loss: 58.9891
====> Test set loss: 58.2526
====> Epoch: 4 Average loss: 58.4252
====> Test set loss: 57.6530
====> Epoch: 5 Average loss: 58.1854
====> Test set loss: 56.7840
====> Epoch: 6 Average loss: 56.7914
====> Test set loss: 56.4034
====> Epoch: 7 Average loss: 56.2514
====> Test set loss: 55.9732
====> Epoch: 8 Average loss: 55.8531
====> Test set loss: 55.5329
====> Epoch: 9 Average loss: 55.5975
====> Test set loss: 55.4575
====> Epoch: 10 Average loss: 55.2392
====> Test set loss: 55.0394
====> Epoch: 11 Average loss: 54.9960
====> Test set loss: 54.6947
====> Epoch: 12 Average loss: 54.9550
====> Test set loss: 54.7900
====> Epoch: 13 Average loss: 54.6072
====> Test set loss: 54.1867
====> Epoch: 14 Average loss: 54.4700
====> Test set loss: 54.1064
====> Epoch: 15 Average loss: 54.3396
====> Test set loss: 54.1945
====> Epoch: 16 Average loss: 54.1721
====> Test set loss: 54.0679
====> Epoch: 17 Average loss: 55.0234
====> Test set loss: 54.8609
====> Epoch: 18 Average loss: 54.2472
====> Test set loss: 54.1380
====> Epoch: 19 Average loss: 53.9772
====> Test set loss: 53.7722
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{vae}\PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{vae\PYZus{}gan\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{,} \PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{fake\PYZus{}output}\PY{p}{)}\PY{p}{:}
    \PY{n}{VAE\PYZus{}loss} \PY{o}{=} \PY{n}{vae\PYZus{}loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
    \PY{n}{adv\PYZus{}loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{fake\PYZus{}output}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{fake\PYZus{}output}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{VAE\PYZus{}loss} \PY{o}{+} \PY{n}{adv\PYZus{}loss}

\PY{k}{def} \PY{n+nf}{discriminator\PYZus{}loss}\PY{p}{(}\PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{fake\PYZus{}output}\PY{p}{)}\PY{p}{:}
    \PY{n}{real\PYZus{}loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{real\PYZus{}output}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{real\PYZus{}output}\PY{p}{)}\PY{p}{)}
    \PY{n}{fake\PYZus{}loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{fake\PYZus{}output}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{fake\PYZus{}output}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{real\PYZus{}loss} \PY{o}{+} \PY{n}{fake\PYZus{}loss}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{discriminator} \PY{o}{=} \PY{n}{DiscriminatorWithSpectralNorm}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{vae\PYZus{}gan} \PY{o}{=} \PY{n}{VAEGAN}\PY{p}{(}\PY{n}{vae}\PY{p}{,} \PY{n}{discriminator}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{n}{train\PYZus{}vae\PYZus{}gan}\PY{p}{(}\PY{n}{vae\PYZus{}gan}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch [0/20], d\_loss: 1.3313, g\_loss: 3297.3393
Epoch [1/20], d\_loss: 1.2736, g\_loss: 3266.9220
Epoch [2/20], d\_loss: 1.2468, g\_loss: 3257.5763
Epoch [3/20], d\_loss: 1.2326, g\_loss: 3250.6384
Epoch [4/20], d\_loss: 1.2242, g\_loss: 3245.9573
Epoch [5/20], d\_loss: 1.2194, g\_loss: 3239.6213
Epoch [6/20], d\_loss: 1.2165, g\_loss: 3237.9134
Epoch [7/20], d\_loss: 1.2138, g\_loss: 3234.7144
Epoch [8/20], d\_loss: 1.2123, g\_loss: 3233.8741
Epoch [9/20], d\_loss: 1.2124, g\_loss: 3233.8619
Epoch [10/20], d\_loss: 1.2108, g\_loss: 3230.4439
Epoch [11/20], d\_loss: 1.2100, g\_loss: 3226.6941
Epoch [12/20], d\_loss: 1.2093, g\_loss: 3226.1228
Epoch [13/20], d\_loss: 1.2083, g\_loss: 3223.7569
Epoch [14/20], d\_loss: 1.2077, g\_loss: 3220.5416
Epoch [15/20], d\_loss: 1.2073, g\_loss: 3219.8003
Epoch [16/20], d\_loss: 1.2072, g\_loss: 3218.8582
Epoch [17/20], d\_loss: 1.2065, g\_loss: 3216.2781
Epoch [18/20], d\_loss: 1.2062, g\_loss: 3215.3682
Epoch [19/20], d\_loss: 1.2064, g\_loss: 3214.3161
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{use\PYZus{}pca}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} 生成样本}
        \PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{vae}\PY{o}{.}\PY{n}{latent\PYZus{}dim}  \PY{c+c1}{\PYZsh{} 通过 VAEGAN 的 VAE 访问 latent\PYZus{}dim}
        \PY{n}{z} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{samples} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{vae}\PY{o}{.}\PY{n}{generate}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 使用 VAE 的 generate 方法}

        \PY{c+c1}{\PYZsh{} 可视化生成样本}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
        \PY{n}{grid\PYZus{}img} \PY{o}{=} \PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{samples}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,} \PY{n}{nrow}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{grid\PYZus{}img}\PY{o}{.}\PY{n}{permute}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} 可视化潜在空间}
        \PY{n}{latents} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{labels} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{for} \PY{n}{data}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{data\PYZus{}loader}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{vae}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 使用 VAE 的 encode 方法}
            \PY{n}{z}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{vae}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 使用 VAE 的 reparameterize 方法}
            \PY{n}{latents}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{z}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{labels}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{label}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}

        \PY{n}{latents} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
        \PY{n}{labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{labels}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} 如果潜在空间维度 \PYZgt{} 2，使用降维}
        \PY{k}{if} \PY{n}{latent\PYZus{}dim} \PY{o}{\PYZgt{}} \PY{l+m+mi}{2}\PY{p}{:}
            \PY{k}{if} \PY{n}{use\PYZus{}pca}\PY{p}{:}
                \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{TSNE}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{latents}  \PY{c+c1}{\PYZsh{} 如果 latent\PYZus{}dim 正好是 2}

        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{latents\PYZus{}2d}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{latents\PYZus{}2d}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{labels}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Dimension 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Dimension 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Space Visualization}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{vae\PYZus{}gan}\PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_52_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_52_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Key Findings}\label{key-findings}

\paragraph{\texorpdfstring{1. \textbf{Sample
Quality}}{1. Sample Quality}}\label{sample-quality}

\begin{itemize}
\item
  \textbf{Blurry and Distorted}: Generated samples are unclear and often
  distorted, with poor boundary definition.
\item
  \textbf{Potential Issues}: Insufficient training and imbalance between
  adversarial and reconstruction losses.
\item
  \textbf{Suggestions}:

  \begin{itemize}
  \tightlist
  \item
    Extend training time.
  \item
    Rebalance loss weights.
  \item
    Enhance discriminator complexity.
  \end{itemize}
\end{itemize}

\paragraph{\texorpdfstring{2. \textbf{Latent Space
Visualization}}{2. Latent Space Visualization}}\label{latent-space-visualization}

\begin{itemize}
\item
  \textbf{Weak Category Separation}: Data points are evenly distributed
  but lack clear category separation.
\item
  \textbf{Potential Issues}: Inadequate latent space representation and
  disruption from adversarial training.
\item
  \textbf{Suggestions}:

  \begin{itemize}
  \tightlist
  \item
    Increase KL divergence weight.
  \item
    Add flow model complexity.
  \item
    Apply stronger regularization.
  \end{itemize}
\end{itemize}

\paragraph{\texorpdfstring{3. \textbf{Next
Steps}}{3. Next Steps}}\label{next-steps}

\begin{itemize}
\tightlist
\item
  \textbf{Improve Sample Quality}: Continue training, adjust loss
  weights, and enhance the discriminator.
\item
  \textbf{Refine Latent Space}: Increase model complexity to achieve
  better category separation and consistency.
\end{itemize}

    \section{Report: Overview of
MultivariateGKDistribution}\label{report-overview-of-multivariategkdistribution}

\textbf{MultivariateGKDistribution} is a specialized parameterized
distribution model designed to extend the univariate g-and-k
distribution for multivariate scenarios. It is particularly effective in
capturing the complexities of real-world data, such as asymmetry and
heavy-tailed characteristics.

\paragraph{Key Parameters:}\label{key-parameters}

\begin{itemize}
\tightlist
\item
  \textbf{Location (a)}: Centers the distribution.
\item
  \textbf{Scale (b)}: Determines the distribution's spread.
\item
  \textbf{Skewness (g)}: Controls the asymmetry of the distribution.
\item
  \textbf{Kurtosis (k)}: Governs the thickness of the distribution's
  tails, impacting its heavy-tailed nature.
\end{itemize}

\paragraph{Mathematical Foundation:}\label{mathematical-foundation}

The distribution is defined by applying a nonlinear transformation to a
standard normal variable \texttt{u}, producing a variable \texttt{z}
whose distribution is shaped by the parameters \texttt{a}, \texttt{b},
\texttt{g}, and \texttt{k}. This flexibility allows the model to
represent a wide range of distributional forms, from symmetric normal
distributions to skewed and heavy-tailed distributions.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}

\PY{k}{class} \PY{n+nc}{MultivariateGKDistribution}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{dim}\PY{p}{,} \PY{n}{a}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{b}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{g}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{MultivariateGKDistribution}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim} \PY{o}{=} \PY{n}{dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{k} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{u}\PY{p}{)}\PY{p}{:}
        \PY{n}{z} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{l+m+mf}{0.8} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{u}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{k} \PY{o}{*} \PY{n}{u}
        \PY{k}{return} \PY{n}{z}

    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        \PY{n}{u} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{u}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{optimize} \PY{k+kn}{import} \PY{n}{least\PYZus{}squares}

\PY{k}{def} \PY{n+nf}{levenberg\PYZus{}marquardt\PYZus{}loss}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{:}
    \PY{n}{dim} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{dim}
    \PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{dim}\PY{p}{:}\PY{p}{]}
    \PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}

    \PY{n}{samples} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{)}
    \PY{n}{loss} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{samples} \PY{o}{\PYZhy{}} \PY{n}{target\PYZus{}distribution}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
    \PY{k}{return} \PY{n}{loss}

\PY{k}{def} \PY{n+nf}{optimize\PYZus{}gk\PYZus{}parameters}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{:}
    \PY{n}{initial\PYZus{}params} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
    \PY{n}{result} \PY{o}{=} \PY{n}{least\PYZus{}squares}\PY{p}{(}\PY{n}{levenberg\PYZus{}marquardt\PYZus{}loss}\PY{p}{,} \PY{n}{initial\PYZus{}params}\PY{p}{,} \PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{,} \PY{n}{method}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{optimized\PYZus{}params} \PY{o}{=} \PY{n}{result}\PY{o}{.}\PY{n}{x}
    
    \PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{FlowLayer}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{FlowLayer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Adjusted to have a singleton dimension for broadcasting}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{linear} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b}
        \PY{n}{activation} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{linear}\PY{p}{)}
        \PY{n}{z\PYZus{}new} \PY{o}{=} \PY{n}{z} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{*} \PY{n}{activation}  \PY{c+c1}{\PYZsh{} Broadcast u}

        \PY{n}{psi} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{activation}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Squeeze w to match the psi calculation}
        \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{psi}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{p}{)}\PY{p}{)}

        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{det\PYZus{}jacobian} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}

        \PY{k}{return} \PY{n}{z\PYZus{}new}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

\PY{k}{class} \PY{n+nc}{NormalizingFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{base\PYZus{}dist}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{NormalizingFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{FlowLayer}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}flows}\PY{p}{)}\PY{p}{]}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}dist} \PY{o}{=} \PY{n}{base\PYZus{}dist} \PY{k}{if} \PY{n}{base\PYZus{}dist} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{k}{else} \PY{n}{MultivariateGKDistribution}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{z}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{z}\PY{o}{.}\PY{n}{device}\PY{p}{)}
        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{log\PYZus{}det}
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}
    
    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        \PY{n}{z0} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}dist}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{z0}\PY{p}{)}
        \PY{k}{return} \PY{n}{z}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{from} \PY{n+nn}{torchvision} \PY{k+kn}{import} \PY{n}{datasets}\PY{p}{,} \PY{n}{transforms}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{DataLoader}

\PY{k}{class} \PY{n+nc}{VAEWithFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEWithFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{latent\PYZus{}dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}  \PY{c+c1}{\PYZsh{} For mean and log variance}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Sigmoid}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Apply sigmoid to ensure output is in range [0, 1]}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow} \PY{o}{=} \PY{n}{NormalizingFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{n}{num\PYZus{}flows}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Flatten the input image to match the input\PYZus{}dim of 784}
        \PY{n}{params} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{:}\PY{p}{]}
        \PY{k}{return} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{n}{z} \PY{o}{=} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder}\PY{p}{(}\PY{n}{z}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Keep it as [batch\PYZus{}size, 784], values in [0, 1]}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{recon\PYZus{}x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}
        \PY{k}{return} \PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{generate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Use the decoder to generate images from z}

    
    \PY{c+c1}{\PYZsh{} Define the MNIST dataset with the correct transformation}
\PY{n}{transform} \PY{o}{=} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
    \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Converts the image to a tensor with pixel values between 0 and 1}
\PY{p}{]}\PY{p}{)}

\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Initialize the VAE model with Normalizing Flow}
\PY{n}{vae\PYZus{}with\PYZus{}flow} \PY{o}{=} \PY{n}{VAEWithFlow}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the optimizer}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Flatten the input to match the output of the decoder}
    \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Flatten target to [batch\PYZus{}size, 784]}
    
    \PY{n}{BCE} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{functional}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}

\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{batch\PYZus{}idx}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{)}\PY{p}{:}
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        
       
        \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, Loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.6f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1, Loss: 165.520616
Epoch 2, Loss: 128.902781
Epoch 3, Loss: 117.573229
Epoch 4, Loss: 112.241660
Epoch 5, Loss: 109.010816
Epoch 6, Loss: 106.742873
Epoch 7, Loss: 105.046126
Epoch 8, Loss: 103.527843
Epoch 9, Loss: 102.124691
Epoch 10, Loss: 100.774069
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k+kn}{import} \PY{n}{make\PYZus{}grid}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k+kn}{import} \PY{n}{PCA}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{manifold} \PY{k+kn}{import} \PY{n}{TSNE}

\PY{k}{def} \PY{n+nf}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{data\PYZus{}loader}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{use\PYZus{}pca}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
    \PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{latent\PYZus{}dim}  \PY{c+c1}{\PYZsh{} }
    
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} }
        \PY{n}{z} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{samples} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{generate}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)} 

       
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
        \PY{n}{grid\PYZus{}img} \PY{o}{=} \PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{samples}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,} \PY{n}{nrow}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{grid\PYZus{}img}\PY{o}{.}\PY{n}{permute}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

       
        \PY{n}{latents} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{labels} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{for} \PY{n}{data}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n}{data\PYZus{}loader}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}\PY{p}{)}  
            \PY{n}{z}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}  
            \PY{n}{latents}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{z}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{labels}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{label}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}

        \PY{n}{latents} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
        \PY{n}{labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{labels}\PY{p}{)}

       
        \PY{k}{if} \PY{n}{latent\PYZus{}dim} \PY{o}{\PYZgt{}} \PY{l+m+mi}{2}\PY{p}{:}
            \PY{k}{if} \PY{n}{use\PYZus{}pca}\PY{p}{:}
                \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{TSNE}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{latents}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{latents\PYZus{}2d} \PY{o}{=} \PY{n}{latents} 

        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{latents\PYZus{}2d}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{latents\PYZus{}2d}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{labels}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Dimension 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Dimension 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Latent Space Visualization}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}

\PY{k}{class} \PY{n+nc}{MultivariateGKDistribution}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{dim}\PY{p}{,} \PY{n}{a}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{b}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{g}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{MultivariateGKDistribution}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim} \PY{o}{=} \PY{n}{dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{k} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{u}\PY{p}{)}\PY{p}{:}
        \PY{n}{z} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{l+m+mf}{0.8} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{u}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{k} \PY{o}{*} \PY{n}{u}
        \PY{k}{return} \PY{n}{z}

    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        \PY{n}{u} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{u}\PY{p}{)}
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{optimize} \PY{k+kn}{import} \PY{n}{least\PYZus{}squares}

\PY{k}{def} \PY{n+nf}{levenberg\PYZus{}marquardt\PYZus{}loss}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{:}
    \PY{n}{dim} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{dim}
    \PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{dim}\PY{p}{:}\PY{p}{]}
    \PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}

    \PY{n}{samples} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{)}
    \PY{n}{loss} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{samples} \PY{o}{\PYZhy{}} \PY{n}{target\PYZus{}distribution}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
    \PY{k}{return} \PY{n}{loss}

\PY{k}{def} \PY{n+nf}{optimize\PYZus{}gk\PYZus{}parameters}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{:}
    \PY{n}{initial\PYZus{}params} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
    \PY{n}{result} \PY{o}{=} \PY{n}{least\PYZus{}squares}\PY{p}{(}\PY{n}{levenberg\PYZus{}marquardt\PYZus{}loss}\PY{p}{,} \PY{n}{initial\PYZus{}params}\PY{p}{,} \PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{,} \PY{n}{method}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{optimized\PYZus{}params} \PY{o}{=} \PY{n}{result}\PY{o}{.}\PY{n}{x}
    
    \PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
\PY{k}{class} \PY{n+nc}{FlowLayer}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{FlowLayer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Adjusted to have a singleton dimension for broadcasting}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{linear} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b}
        \PY{n}{activation} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{linear}\PY{p}{)}
        \PY{n}{z\PYZus{}new} \PY{o}{=} \PY{n}{z} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{*} \PY{n}{activation}  \PY{c+c1}{\PYZsh{} Broadcast u}

        \PY{n}{psi} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{activation}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Squeeze w to match the psi calculation}
        \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{psi}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{p}{)}\PY{p}{)}

        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{det\PYZus{}jacobian} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}

        \PY{k}{return} \PY{n}{z\PYZus{}new}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

\PY{k}{class} \PY{n+nc}{NormalizingFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{base\PYZus{}dist}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{NormalizingFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{FlowLayer}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}flows}\PY{p}{)}\PY{p}{]}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}dist} \PY{o}{=} \PY{n}{base\PYZus{}dist} \PY{k}{if} \PY{n}{base\PYZus{}dist} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{k}{else} \PY{n}{MultivariateGKDistribution}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{z}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{z}\PY{o}{.}\PY{n}{device}\PY{p}{)}
        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{log\PYZus{}det}
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}
    
    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        \PY{n}{z0} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}dist}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{z0}\PY{p}{)}
        \PY{k}{return} \PY{n}{z}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{from} \PY{n+nn}{torchvision} \PY{k+kn}{import} \PY{n}{datasets}\PY{p}{,} \PY{n}{transforms}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{DataLoader}

\PY{k}{class} \PY{n+nc}{VAEWithFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEWithFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{latent\PYZus{}dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}  \PY{c+c1}{\PYZsh{} For mean and log variance}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Sigmoid}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Apply sigmoid to ensure output is in range [0, 1]}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow} \PY{o}{=} \PY{n}{NormalizingFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{n}{num\PYZus{}flows}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Flatten the input image to match the input\PYZus{}dim of 784}
        \PY{n}{params} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{:}\PY{p}{]}
        \PY{k}{return} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{n}{z} \PY{o}{=} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder}\PY{p}{(}\PY{n}{z}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Keep it as [batch\PYZus{}size, 784], values in [0, 1]}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{recon\PYZus{}x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}
        \PY{k}{return} \PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{generate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Use the decoder to generate images from z}

    
    \PY{c+c1}{\PYZsh{} Define the MNIST dataset with the correct transformation}
\PY{n}{transform} \PY{o}{=} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
    \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}  \PY{c+c1}{\PYZsh{} Converts the image to a tensor with pixel values between 0 and 1}
\PY{p}{]}\PY{p}{)}

\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Initialize the VAE model with Normalizing Flow}
\PY{n}{vae\PYZus{}with\PYZus{}flow} \PY{o}{=} \PY{n}{VAEWithFlow}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the optimizer}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Flatten the input to match the output of the decoder}
    \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Flatten target to [batch\PYZus{}size, 784]}
    
    \PY{n}{BCE} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{functional}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}

\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{for} \PY{n}{batch\PYZus{}idx}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{)}\PY{p}{:}
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        
       
        \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, Loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.6f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1, Loss: 170.042920
Epoch 2, Loss: 133.808491
Epoch 3, Loss: 121.290844
Epoch 4, Loss: 114.870641
Epoch 5, Loss: 111.416504
Epoch 6, Loss: 109.153973
Epoch 7, Loss: 107.344833
Epoch 8, Loss: 105.460621
Epoch 9, Loss: 103.956071
Epoch 10, Loss: 102.798736
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Example usage after training:}
\PY{n}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_62_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_62_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Potential spatial distribution generated using training data}
\PY{n}{latent\PYZus{}samples} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
\PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{train\PYZus{}loader}\PY{p}{:}
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n}{z} \PY{o}{=} \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{latent\PYZus{}samples}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{z}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{target\PYZus{}distribution} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{latent\PYZus{}samples}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{from} \PY{n+nn}{torchvision} \PY{k+kn}{import} \PY{n}{datasets}\PY{p}{,} \PY{n}{transforms}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{DataLoader}
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{optimize} \PY{k+kn}{import} \PY{n}{least\PYZus{}squares}

\PY{c+c1}{\PYZsh{} Multivariate g\PYZhy{}k Distribution}
\PY{k}{class} \PY{n+nc}{MultivariateGKDistribution}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{dim}\PY{p}{,} \PY{n}{a}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{b}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{g}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{MultivariateGKDistribution}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim} \PY{o}{=} \PY{n}{dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{k} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{u}\PY{p}{)}\PY{p}{:}
        \PY{n}{z} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{l+m+mf}{0.8} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{u}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{k} \PY{o}{*} \PY{n}{u}
        \PY{k}{return} \PY{n}{z}

    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        \PY{n}{u} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{u}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Levenberg\PYZhy{}Marquardt Loss Function}
\PY{k}{def} \PY{n+nf}{levenberg\PYZus{}marquardt\PYZus{}loss}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{:}
    \PY{n}{dim} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{dim}
    \PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{dim}\PY{p}{:}\PY{p}{]}
    \PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}

    \PY{n}{samples} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{)}
    \PY{n}{loss} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{samples} \PY{o}{\PYZhy{}} \PY{n}{target\PYZus{}distribution}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
    \PY{k}{return} \PY{n}{loss}

\PY{k}{def} \PY{n+nf}{optimize\PYZus{}gk\PYZus{}parameters}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{:}
    \PY{n}{initial\PYZus{}params} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 使用 \PYZsq{}trf\PYZsq{} 方法替代 \PYZsq{}lm\PYZsq{}，因为 \PYZsq{}trf\PYZsq{} 没有 \PYZsq{}lm\PYZsq{} 的限制}
    \PY{n}{result} \PY{o}{=} \PY{n}{least\PYZus{}squares}\PY{p}{(}\PY{n}{levenberg\PYZus{}marquardt\PYZus{}loss}\PY{p}{,} \PY{n}{initial\PYZus{}params}\PY{p}{,} \PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{,} \PY{n}{method}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    
    \PY{n}{optimized\PYZus{}params} \PY{o}{=} \PY{n}{result}\PY{o}{.}\PY{n}{x}
    
    \PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}

    
    
\PY{c+c1}{\PYZsh{} Flow Layer and Normalizing Flow}
\PY{k}{class} \PY{n+nc}{FlowLayer}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{FlowLayer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{linear} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b}
        \PY{n}{activation} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{linear}\PY{p}{)}
        \PY{n}{z\PYZus{}new} \PY{o}{=} \PY{n}{z} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{*} \PY{n}{activation}

        \PY{n}{psi} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{activation}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{psi}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{p}{)}\PY{p}{)}

        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{det\PYZus{}jacobian} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}

        \PY{k}{return} \PY{n}{z\PYZus{}new}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

\PY{k}{class} \PY{n+nc}{NormalizingFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{base\PYZus{}dist}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{NormalizingFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{FlowLayer}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}flows}\PY{p}{)}\PY{p}{]}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}dist} \PY{o}{=} \PY{n}{base\PYZus{}dist} \PY{k}{if} \PY{n}{base\PYZus{}dist} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{k}{else} \PY{n}{MultivariateGKDistribution}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{z}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{z}\PY{o}{.}\PY{n}{device}\PY{p}{)}
        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{log\PYZus{}det}
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}
    
    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        \PY{n}{z0} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}dist}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{z0}\PY{p}{)}
        \PY{k}{return} \PY{n}{z}

\PY{c+c1}{\PYZsh{} VAE with Normalizing Flow and g\PYZhy{}k Distribution}
\PY{k}{class} \PY{n+nc}{VAEWithFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEWithFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{latent\PYZus{}dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}  \PY{c+c1}{\PYZsh{} For mean and log variance}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Sigmoid}\PY{p}{(}\PY{p}{)}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow} \PY{o}{=} \PY{n}{NormalizingFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{n}{num\PYZus{}flows}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}
        \PY{n}{params} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{:}\PY{p}{]}
        \PY{k}{return} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{n}{z} \PY{o}{=} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder}\PY{p}{(}\PY{n}{z}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{recon\PYZus{}x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}
        \PY{k}{return} \PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{generate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the MNIST dataset with the correct transformation}
\PY{n}{transform} \PY{o}{=} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
    \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
\PY{p}{]}\PY{p}{)}

\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Initialize the VAE model with Normalizing Flow and g\PYZhy{}k distribution}
\PY{n}{vae\PYZus{}with\PYZus{}flow} \PY{o}{=} \PY{n}{VAEWithFlow}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Levenberg\PYZhy{}Marquardt optimization of g\PYZhy{}k distribution before training}
\PY{c+c1}{\PYZsh{} Here, we assume target\PYZus{}distribution is some pre\PYZhy{}defined target data you wish to fit the g\PYZhy{}k distribution to}
\PY{c+c1}{\PYZsh{} Replace with actual target distribution data}
\PY{n}{optimize\PYZus{}gk\PYZus{}parameters}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{flow}\PY{o}{.}\PY{n}{base\PYZus{}dist}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the optimizer}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}
    \PY{n}{BCE} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{functional}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}

\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{batch\PYZus{}idx}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{)}\PY{p}{:}
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, Loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.6f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1, Loss: 179.468242
Epoch 2, Loss: 155.490709
Epoch 3, Loss: 143.635647
Epoch 4, Loss: 135.820261
Epoch 5, Loss: 140.466578
Epoch 6, Loss: 140.380552
Epoch 7, Loss: 138.572246
Epoch 8, Loss: 133.961569
Epoch 9, Loss: 133.757182
Epoch 10, Loss: 144.803902
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define the MNIST dataset with the correct transformation}
\PY{n}{transform} \PY{o}{=} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
    \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
\PY{p}{]}\PY{p}{)}

\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Initialize the VAE model with Normalizing Flow and g\PYZhy{}k distribution}
\PY{n}{vae\PYZus{}with\PYZus{}flow} \PY{o}{=} \PY{n}{VAEWithFlow}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Levenberg\PYZhy{}Marquardt optimization of g\PYZhy{}k distribution before training}
\PY{c+c1}{\PYZsh{} Here, we assume target\PYZus{}distribution is some pre\PYZhy{}defined target data you wish to fit the g\PYZhy{}k distribution to}

\PY{n}{optimize\PYZus{}gk\PYZus{}parameters}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{flow}\PY{o}{.}\PY{n}{base\PYZus{}dist}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the optimizer}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}
    \PY{n}{BCE} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{functional}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}

\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{batch\PYZus{}idx}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{)}\PY{p}{:}
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, Loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.6f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1, Loss: 179.615487
Epoch 2, Loss: 151.809655
Epoch 3, Loss: 146.450001
Epoch 4, Loss: 142.862890
Epoch 5, Loss: 140.388594
Epoch 6, Loss: 153.785822
Epoch 7, Loss: 139.039761
Epoch 8, Loss: 134.541962
Epoch 9, Loss: 136.854830
Epoch 10, Loss: 131.666251
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 调用可视化函数}
\PY{n}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Advantages of This Plot:}\label{advantages-of-this-plot}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Clear Boundaries}:

  \begin{itemize}
  \tightlist
  \item
    Distinct classification boundaries suggest well-defined regions for
    different digits in the latent space, aiding clear differentiation
    during generation.
  \end{itemize}
\item
  \textbf{Concentrated Regions}:

  \begin{itemize}
  \tightlist
  \item
    The data points are more concentrated, indicating deeper recognition
    of specific digits, which supports generating distinct,
    well-separated digits.
  \end{itemize}
\item
  \textbf{Better Separation}:

  \begin{itemize}
  \tightlist
  \item
    The clear boundaries and dispersed distribution help the model
    better identify and generate each digit.
  \end{itemize}
\end{enumerate}

\subsubsection{Features of the Previous
Plot:}\label{features-of-the-previous-plot}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Broader Coverage}:

  \begin{itemize}
  \tightlist
  \item
    The previous plot shows an even distribution over a larger latent
    space, which might help generate diverse samples but could reduce
    classification clarity.
  \end{itemize}
\item
  \textbf{Higher Ambiguity}:

  \begin{itemize}
  \tightlist
  \item
    The lack of distinct boundaries in the previous plot may lead to
    less clear differentiation between digits.
  \end{itemize}
\end{enumerate}

\subsubsection{Conclusion:}\label{conclusion}

\textbf{This plot} better aligns with our goal of clearly distinguishing
digits, with superior classification boundaries and concentrated
regions, making it more effective for our needs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{from} \PY{n+nn}{torchvision} \PY{k+kn}{import} \PY{n}{datasets}\PY{p}{,} \PY{n}{transforms}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{DataLoader}
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{optimize} \PY{k+kn}{import} \PY{n}{least\PYZus{}squares}

\PY{c+c1}{\PYZsh{} Multivariate g\PYZhy{}k Distribution}
\PY{k}{class} \PY{n+nc}{MultivariateGKDistribution}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{dim}\PY{p}{,} \PY{n}{a}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{b}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{g}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{MultivariateGKDistribution}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim} \PY{o}{=} \PY{n}{dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{k} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{repeat}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{u}\PY{p}{)}\PY{p}{:}
        \PY{n}{z} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{l+m+mf}{0.8} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{g} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{u}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{k} \PY{o}{*} \PY{n}{u}
        \PY{k}{return} \PY{n}{z}

    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        \PY{n}{u} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{u}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Levenberg\PYZhy{}Marquardt Loss Function}
\PY{k}{def} \PY{n+nf}{levenberg\PYZus{}marquardt\PYZus{}loss}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{:}
    \PY{n}{dim} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{dim}
    \PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{dim}\PY{p}{:}\PY{p}{]}
    \PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}

    \PY{n}{samples} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{)}
    \PY{n}{loss} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{samples} \PY{o}{\PYZhy{}} \PY{n}{target\PYZus{}distribution}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
    \PY{k}{return} \PY{n}{loss}

\PY{k}{def} \PY{n+nf}{optimize\PYZus{}gk\PYZus{}parameters}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{:}
    \PY{n}{initial\PYZus{}params} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{result} \PY{o}{=} \PY{n}{least\PYZus{}squares}\PY{p}{(}\PY{n}{levenberg\PYZus{}marquardt\PYZus{}loss}\PY{p}{,} \PY{n}{initial\PYZus{}params}\PY{p}{,} \PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}\PY{p}{,} \PY{n}{method}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    
    \PY{n}{optimized\PYZus{}params} \PY{o}{=} \PY{n}{result}\PY{o}{.}\PY{n}{x}
    
    \PY{n}{model}\PY{o}{.}\PY{n}{a}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{p}{:}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{b}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{g}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{k}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{optimized\PYZus{}params}\PY{p}{[}\PY{l+m+mi}{3}\PY{o}{*}\PY{n}{model}\PY{o}{.}\PY{n}{dim}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Flow Layers and Normalizing Flow}
\PY{k}{class} \PY{n+nc}{FlowLayer}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{FlowLayer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{linear} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b}
        \PY{n}{activation} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{linear}\PY{p}{)}
        \PY{n}{z\PYZus{}new} \PY{o}{=} \PY{n}{z} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{*} \PY{n}{activation}

        \PY{n}{psi} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{activation}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{psi}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{u}\PY{p}{)}\PY{p}{)}

        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{det\PYZus{}jacobian} \PY{o}{+} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}

        \PY{k}{return} \PY{n}{z\PYZus{}new}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

\PY{c+c1}{\PYZsh{} Introducing Radial Flow for more complex transformations}
\PY{k}{class} \PY{n+nc}{RadialFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{RadialFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{z0} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{beta} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{r} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{z} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{z0}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdim}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{h} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha} \PY{o}{+} \PY{n}{r}\PY{p}{)}
        \PY{n}{beta\PYZus{}h} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{beta} \PY{o}{*} \PY{n}{h}
        \PY{n}{z\PYZus{}new} \PY{o}{=} \PY{n}{z} \PY{o}{+} \PY{n}{beta\PYZus{}h} \PY{o}{*} \PY{p}{(}\PY{n}{z} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{z0}\PY{p}{)}
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{beta\PYZus{}h} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{beta\PYZus{}h} \PY{o}{*} \PY{n}{r}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{k}{return} \PY{n}{z\PYZus{}new}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

\PY{k}{class} \PY{n+nc}{NormalizingFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{base\PYZus{}dist}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{NormalizingFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} 使用更加复杂的流结构，结合不同类型的流}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}
            \PY{p}{[}\PY{n}{FlowLayer}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{if} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{2} \PY{o}{==} \PY{l+m+mi}{0} \PY{k}{else} \PY{n}{RadialFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}flows}\PY{p}{)}\PY{p}{]}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}dist} \PY{o}{=} \PY{n}{base\PYZus{}dist} \PY{k}{if} \PY{n}{base\PYZus{}dist} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{k}{else} \PY{n}{MultivariateGKDistribution}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{z}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{z}\PY{o}{.}\PY{n}{device}\PY{p}{)}
        \PY{k}{for} \PY{n}{flow} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flows}\PY{p}{:}
            \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det} \PY{o}{=} \PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
            \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{+}\PY{o}{=} \PY{n}{log\PYZus{}det}
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}
    
    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        \PY{n}{z0} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{base\PYZus{}dist}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{num\PYZus{}samples}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{z0}\PY{p}{)}
        \PY{k}{return} \PY{n}{z}

\PY{c+c1}{\PYZsh{} VAE with Complex Normalizing Flow and g\PYZhy{}k Distribution}
\PY{k}{class} \PY{n+nc}{VAEWithFlow}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{:}  \PY{c+c1}{\PYZsh{} 增加 num\PYZus{}flows}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VAEWithFlow}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim} \PY{o}{=} \PY{n}{latent\PYZus{}dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{latent\PYZus{}dim} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}  \PY{c+c1}{\PYZsh{} For mean and log variance}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Sigmoid}\PY{p}{(}\PY{p}{)}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow} \PY{o}{=} \PY{n}{NormalizingFlow}\PY{p}{(}\PY{n}{latent\PYZus{}dim}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{n}{num\PYZus{}flows}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 使用复杂的流结构}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}
        \PY{n}{params} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{]}\PY{p}{,} \PY{n}{params}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{latent\PYZus{}dim}\PY{p}{:}\PY{p}{]}
        \PY{k}{return} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}

    \PY{k}{def} \PY{n+nf}{reparameterize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}\PY{p}{:}
        \PY{n}{std} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{eps} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn\PYZus{}like}\PY{p}{(}\PY{n}{std}\PY{p}{)}
        \PY{n}{z} \PY{o}{=} \PY{n}{mu} \PY{o}{+} \PY{n}{eps} \PY{o}{*} \PY{n}{std}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{z}\PY{p}{)}
        \PY{k}{return} \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder}\PY{p}{(}\PY{n}{z}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mu}\PY{p}{,} \PY{n}{logvar} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{z}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{reparameterize}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{)}
        \PY{n}{recon\PYZus{}x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}
        \PY{k}{return} \PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}

    \PY{k}{def} \PY{n+nf}{generate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the MNIST dataset with the correct transformation}
\PY{n}{transform} \PY{o}{=} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
    \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
\PY{p}{]}\PY{p}{)}

\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{MNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Initialize the VAE model with Complex Normalizing Flow and g\PYZhy{}k distribution}
\PY{n}{vae\PYZus{}with\PYZus{}flow} \PY{o}{=} \PY{n}{VAEWithFlow}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{784}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,} \PY{n}{latent\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{num\PYZus{}flows}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Levenberg\PYZhy{}Marquardt optimization of g\PYZhy{}k distribution before training}
\PY{c+c1}{\PYZsh{} Here, we assume target\PYZus{}distribution is some pre\PYZhy{}defined target data you wish to fit the g\PYZhy{}k distribution to}
\PY{c+c1}{\PYZsh{} Replace with actual target distribution data}
\PY{n}{optimize\PYZus{}gk\PYZus{}parameters}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{flow}\PY{o}{.}\PY{n}{base\PYZus{}dist}\PY{p}{,} \PY{n}{target\PYZus{}distribution}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the optimizer}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{784}\PY{p}{)}
    \PY{n}{BCE} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{functional}\PY{o}{.}\PY{n}{binary\PYZus{}cross\PYZus{}entropy}\PY{p}{(}\PY{n}{recon\PYZus{}x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{KLD} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{logvar} \PY{o}{\PYZhy{}} \PY{n}{mu}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{logvar}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{BCE} \PY{o}{+} \PY{n}{KLD} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}

\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
    \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{batch\PYZus{}idx}\PY{p}{,} \PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{)}\PY{p}{:}
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian} \PY{o}{=} \PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{recon\PYZus{}batch}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{logvar}\PY{p}{,} \PY{n}{log\PYZus{}det\PYZus{}jacobian}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, Loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}loss}\PY{+w}{ }\PY{o}{/}\PY{+w}{ }\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{.6f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1, Loss: 187.225034
Epoch 2, Loss: 159.982319
Epoch 3, Loss: 157.670301
Epoch 4, Loss: 142.617740
Epoch 5, Loss: 147.140170
Epoch 6, Loss: 144.306917
Epoch 7, Loss: 151.251292
Epoch 8, Loss: 145.453654
Epoch 9, Loss: 142.763340
Epoch 10, Loss: 152.582088
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 调用可视化函数}
\PY{n}{visualize\PYZus{}samples\PYZus{}and\PYZus{}latent\PYZus{}space}\PY{p}{(}\PY{n}{vae\PYZus{}with\PYZus{}flow}\PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_69_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_69_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Advantages:}\label{advantages}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Clearer Classification}:

  \begin{itemize}
  \tightlist
  \item
    The data points in this new plot are distinctly grouped into three
    clusters, indicating that the model effectively separates different
    categories (likely different digits) in the latent space. This
    clustered distribution aids in clearer classification of different
    digits.
  \end{itemize}
\item
  \textbf{Higher Separation in Latent Space}:

  \begin{itemize}
  \tightlist
  \item
    The noticeable gaps between clusters suggest a higher degree of
    separation between different categories. This distribution is
    beneficial for accurately distinguishing and recognizing different
    categories during the generation process.
  \end{itemize}
\item
  \textbf{Tight Cluster Distribution}:

  \begin{itemize}
  \tightlist
  \item
    The data points within each cluster are tightly packed, indicating
    the model's high consistency in generating samples of the same
    category, which helps reduce confusion between categories during
    generation.
  \end{itemize}
\end{enumerate}

\subsubsection{Comparison with Previous
Plots:}\label{comparison-with-previous-plots}

\begin{itemize}
\tightlist
\item
  \textbf{Compared to the Previous Plots}:

  \begin{itemize}
  \tightlist
  \item
    The new plot shows more distinct classification, with clearer
    clusters and gaps between them. This suggests that the new model may
    better differentiate between different digits in the generation
    task.
  \end{itemize}
\item
  \textbf{Latent Space Coverage}:

  \begin{itemize}
  \tightlist
  \item
    While the new plot has a less extensive coverage of the latent space
    compared to the previous plots, it offers more explicit and clear
    classification. Therefore, if the goal is to clearly distinguish
    between different categories of digits, the new result might be
    better.
  \end{itemize}
\end{itemize}

\subsubsection{Conclusion:}\label{conclusion}

\textbf{The new plot} performs the best, showing clearer category
separation and tighter intra-category grouping in the latent space. This
is highly advantageous for accurately distinguishing different digits in
the generation task, making the algorithm behind the new plot more
effective. Thank you!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
